\section{Methodology}
\label{sec:methodology}
Our methodology comprises two main stages: (1) unconditional diffusion model training and synthetic image generation, and (2) evaluation of object detection models trained with varying proportions of synthetic data.

\subsection{Image Generation with Diffusion Models}
\label{subsec:image-generation-with-diffusion-models}
We trained an unconditional diffusion model with a U-Net architecture~\cite{ronneberger_u-net_2015} using a dataset of  10,000 patches ($512 \times 512$) extracted from real brightfield microscopy images of stably transfected CHO cells (see supplementary material for dataset details).
The U-Net architecture consisted of down and up-sampling blocks with residual connections and optionally attention mechanisms in the down and up-sampling paths.
We employed the DDIM scheduler~\cite{song_denoising_2020} for efficient sampling and trained the model for 350 epochs using the AdamW optimizer with a learning rate of $1e^{-4}$.
Model selection was based on visual inspection of generated samples and the Fr√©chet Inception Distance (FID) scores calculated against a held-out set of real images.
The final generation process used the Euler a scheduler with trailing timestep spacing and epsilon prediction type, generating images with inference steps randomized between 35 and 40.

\subsection{Cell Detection Model Training and Evaluation}
\label{subsec:cell-detection-model-training-and-evaluation}
We created six datasets containing synthetic images and a baseline dataset containing 5,000 real images (\textit{scc\_real}), that is used to compare the performance of the detection models trained with synthetic data.
Three datasets used varying proportions of synthetic images (10\%, 30\%, and 50\%) to replace real images in the training set (\textit{scc\_10}, \textit{scc\_30}, \textit{scc\_50}), while the remaining three datasets used the same proportions to add synthetic images to the training set (\textit{scc\_add\_10}, \textit{scc\_add\_30}, \textit{scc\_add\_50}).
Real images were labeled using a semi-automated approach combining fluorescence channel information and manual verification.
Synthetic images were labeled using model-assisted labeling with a pre-trained and fine-tuned YOLOv8m model and manual label refinement.

We fine-tuned three state-of-the-art object detection models: YOLOv8 (sizes s, m, x)~\cite{jocher_ultralytics_2023}, YOLOv9 (sizes c, e)~\cite{Wang2024YOLOv9LW}, and RT-DETR (sizes l, x)~\cite{Lv2023DETRsBY}, pre-trained on COCO~\cite{tsung-yi_lin_microsoft_2014}.
Models were trained for 200 epochs using default Ultralytics augmentation settings and evaluated on a held-out test set of 16,758 real images.
Performance was measured using mean Average Precision (mAP) at IoU thresholds of 0.5 (mAP\@50), 0.75 (mAP\@75), and averaged across IoUs from 0.5 to 0.95 (mAP\@50:95).

\subsection{Expert Survey for Image Realism}
\label{subsec:expert-survey-for-image-realism}
To assess the perceptual realism of generated images, we conducted a survey with 11 microscopy experts and biologists from Synentec GmbH\@.
Participants were presented with 30 randomly ordered images (20 synthetic, 10 real) and asked to classify each as real or generated and rate their confidence.
We analyzed classification accuracy and collected textual explanations for misclassifications to understand the visual cues experts relied upon.