##################################################
################### thesis.tex ###################
##################################################

\documentclass[
  a4paper,            % DIN A4
  DIV=10,             % Schriftgröße und Satzspiegel
  oneside,            % einseitiger Druck
  BCOR=5mm,           % Bindungskorrektur
  parskip=half,       % Halber Abstand zwischen Absätzen
  numbers=noenddot,   % Kein Punkt hinter Kapitelnummern
  bibliography=totoc, % Literaturverzeichnis im Inhaltsverzeichnis
  listof=totoc,       % Abbildungs- und Tabellenverzeichnis im Inhaltsverzeichnis
  table
]{scrreprt}
\usepackage{../style/thesisstyle}
\usepackage{enumitem}
%\usepackage{layout}       % Layout Debugging
%\usepackage{showframe}    % Layout Debugging
\usepackage{lipsum}       % for example only
\usepackage{blindtext}
\usepackage{mathtools}    % for example only
\usepackage[fixlanguage]{babelbib}
\usepackage{afterpage}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{csquotes}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}p{#1}}

\makeglossaries           % create all glossary entries (remember: run makeglossaries manually)
\loadglsentries{thesisglossaries.tex}  % load acronym, symbol and glossary entries

\begin{document}
\input{configuration/configuration}    % load all settings

%\layout{}                 % Layout Debugging

\hyphenation{Mas-ter-the-sis}

% Cover page here, no page number
\ICoverPage

% PDF Metadata
\input{../style/metadata}

% Titlepage is page one even if the number is not shown.
\pagenumbering{roman}
% Title page here
\input{../style/titlepage}

% Abstract page here
\input{../style/abstractpage}

% Table of contents here
\tableofcontents

% List of figures here
\IListOfFigures

% List of tables here
\IListOfTables

% List of accronyms here
\IListOfAccronyms

% List of symbols here
\IListOfSymbols

% Uncomment if list of source code is needed (rarely).
%\lstlistoflistings  % requires package listings, needs to uncommenting of usepackage

% path to the chapters folder is set to find the images used there
\graphicspath{ {./chapters/} }

% Chapters
\clearpage
\pagenumbering{arabic}
\input{chapters/01_introduction}
\input{chapters/02_related_works}
\input{chapters/03_background}
\input{chapters/04_motivation}
\input{chapters/05_methodology_unconditional_diffusion}
\input{chapters/06_methodology_object_detection}
\input{chapters/07_results}
\input{chapters/08_discussion}
\input{chapters/09_conclusion}

%\bibliographystyle{plain}
\bibliographystyle{plainnat}
\bibliography{literature}

% Appendix
\appendix
\input{appendix/survey_appendix}
\input{appendix/sample_detections}

\IGlossary

\Istatement

\end{document}

####################################################################
################### chapters/01_introduction.tex ###################
####################################################################
\chapter{Introduction}\label{ch:introduction}
Cell detection is a fundamental task in many biological and medical applications,
serving as a crucial step in understanding cellular processes, diagnosing diseases, and developing new treatments~\cite{meijering_cell_2012}.
The ability to accurately identify and locate individual cells within \gls{gl:brightfield-microscopy} images enables
researchers to quantify cell populations, analyze cell morphology, and track cellular behavior over time.
However, the process of cell detection has traditionally been a labor-intensive and time-consuming task,
often requiring manual annotation by trained domain experts.

\section{Problem Statement}\label{sec:motivation-and-problem-statement}
Recent advancements in \acrfull{ai} and computer vision have shown great promise in automating the cell detection process~\cite{erick_moen_deep_2019}.
For instance, a study by Falk et al.~\cite{thorsten_falk_u-net_2019} demonstrated the successful application of \gls{ai}-assisted cell detection in analyzing
large-scale time-lapse experiments of neural stem cell development.
Their deep learning-based approach not only significantly reduced the time required for analysis but also improved the consistency
and reproducibility of results compared to manual annotation~\cite{thorsten_falk_u-net_2019}.
This example highlights the potential impact of automated cell detection on accelerating biological research and improving the
reliability of experimental outcomes.

Despite the potential benefits of \gls{ai}-assisted cell detection, several challenges remain in developing robust and accurate automated systems.
One of the primary difficulties lies in the inherent variability of cell appearances in microscopy images.
Cells can vary wildly in shape, size, and intensity, depending on factors such as cell type, cell cycle stage, and imaging conditions~\cite{caicedo_data-analysis_2017}.
This variability makes it challenging to develop algorithms that can reliably detect cells across different experimental setups and biological contexts.

Deep learning approaches, particularly \acrfullpl{cnn}, have emerged as powerful tools for addressing the complexities
of cell detection~\cite{xing_robust_2016}.
These models can learn to recognize complex patterns and features directly from raw image data, potentially overcoming the limitations
of traditional handcrafted image processing techniques~\cite{ronneberger_u-net_2015}.
However, the success of deep learning models is heavily dependent on the availability of large, diverse, and accurately labeled datasets for training~\cite{ronneberger_u-net_2015}.

Herein lies the significant bottleneck in the development of automated cell detection systems:
the acquisition and labeling of large-scale microscopy datasets.
Obtaining a sufficient number of high-quality \gls{gl:brightfield-microscopy} images is both expensive and time-consuming, often requiring
specialized equipment and expertise~\cite{pinkard_deep_2019}.
Moreover, the process of manually annotating these images to create ground truth labels for training is equally demanding,
requiring significant time and effort from domain experts.

To address these challenges, researchers have begun exploring the potential of synthetic data generation as a means to augment
over even replace real-world datasets~\cite{rajaram_simucell_2012,trampert_deep_2021,lehmussola_synthetic_2008}.
By leveraging generative models, it may be possible to create large volumes of realistic \gls{gl:brightfield-microscopy} images.


\section{Objective and Scope}\label{sec:objective-and-scope}
The primary objective of this thesis is to evaluate the impact of introducing synthetic \gls{gl:brightfield-microscopy} images into training
datasets for object detection models.
The investigation faces several key challenges, including the generation of high-quality synthetic microscopy images, the training
of various object detection models on datasets containing varying proportions of synthetic data, and the rigorous evaluation
of model performance on real-world test data.

The scope of this research is focused on \gls{gl:brightfield-microscopy} images of single cells, a common and important imaging modality in biological research.
While the ultimate goal is not to maximize detection in absolute terms, the study aims to provide a comparative analysis of how synthetic data impacts
the performance of different state-of-the-art object detection architectures.
This approach will yield insights into the potential benefits and limitations of using synthetic data in the context of cell detection.

\section{Research Questions}\label{sec:research-question}
The central research question driving this thesis can be stated as follows:

\begin{displayquote}
\textbf{RQ: Does the introduction of synthetic \gls{gl:brightfield-microscopy} images to a training dataset positively impact the performance
of object detection models in single cell detection tasks?}
\end{displayquote}

Additionally, the research will explore two related sub-questions:
\begin{enumerate}
    \item SRQ1: Is it possible to generate synthetic \gls{gl:brightfield-microscopy} images to sufficient quality to make them indistinguishable from real images to human experts?
    \item SRQ2: How does the proportion of synthetic data in the training set influence the magnitude and direction of its impact on model performance?
\end{enumerate}
This thesis aims to address these research questions and contribute valuable insights to the fields of computer vision, machine learning,
and computational biology.
The findings may have significant implications for the development of more efficient and effective cell detection systems, potentially
accelerating biological research and improving the accuracy of cellular analysis in various applications.

\section{Thesis Structure}\label{sec:thesis-structure}
The structure of this thesis is designed to provide a comprehensive exploration of the research questions and objectives outlined above.
Following this introduction, Chapter~\ref{ch:related-works} presents a review of relevant literature, covering topics such as
recent advancements in synthetic microscopy image generation and deep learning techniques in microscopy image analysis.
Chapter~\ref{ch:technical-background} provides essential technical background on diffusion models,
different diffusion types and architectures, fundamentals and history of object detection and common metrics used to evaluate object detection models.
Chapter~\ref{ch:motivation} goes into more detail on the motivation behind this research, discussing the current challenges and limitations, potential benefits and
ethical considerations of using \gls{ai}, especially synthetic data, in biological research.
Chapter~\ref{ch:methodology-image-generation} outlines the methodology for generating the synthetic \gls{gl:brightfield-microscopy} images used in this study.
It contains details on the dataset acquisition and preparation, model architectures trained to generate images, training procedures, model evaluation process,
and presents the final model that was selected for generating the synthetic images used in the subsequent experiments.
Chapter~\ref{ch:methodology-cell-detection} builds upon the methodology presented in the previous chapter and describes the process of the cell detection task.
It covers the data labeling, state-of-the-art model selection, training configuration, and describes the evaluation process.
The results of the experiments are presented in Chapter~\ref{ch:results}, including a survey of human experts, visual comparisons of generated and real images,
and quantitative and qualitative assessments of cell detection performance.
Chapter~\ref{ch:discussion} interprets the results, discusses the potential impact of the findings on biological research and applications,
and outlines the limitations and future directions of the research.
Finally, Chapter~\ref{ch:conclusion} summarizes the key findings of the study, provides recommendations for future work, and concludes the thesis.

############################################################
################### chapters/02_related_works.tex ##########
############################################################

\chapter{Related Works}\label{ch:related-works}
Brightfield microscopy is a widely used imaging technique in biological research and medical diagnosis.
However, acquiring high-quality brightfield images can be challenging due to various factors such as
illumination non-uniformity, focusing issues and other laboratory environment settings.
In recent years, there has been growing interest in using artificial intelligence and deep learning approaches to
enhance and analyze microscopy images.
This chapter provides an overview of relevant prior work in two main areas related to this thesis:
\begin{enumerate}
    \item Methods for generating synthetic \gls{gl:brightfield-microscopy} images, and
    \item \gls{ai}-based techniques for microscopy image analysis.
\end{enumerate}
The first section focuses on computational approaches that aim to simulate realistic brightfield images,
which can aid in training deep learning models when real image data is limited.
The second section surveys the application of machine learning, especially deep learning,
for various microscopy image analysis tasks such as cell detection, segmentation and classification.

\section{Synthetic Brightfield Microscopy}\label{sec:synthetic-brightfield-microscopy}
One of the early works in this area by Svoboda et al.~\cite{david_svoboda_generation_2012}, proposed a technique
to generate time-lapse sequences of fully 3D synthetic \gls{gl:brightfield-microscopy} image datasets, including cell shapes, structures and motion.
This enabled the creation of ground truth data for evaluating cell segmentation and tracking algorithms.
They further extended this work~\cite{david_svoboda_towards_2013} to generate more realistic distributions of cell
populations in 3D by controlling cell count and clustering probability.

Several research groups have focused on translating brightfield images into the corresponding \gls{gl:fluorescence-microscopy} images, to reduce the amount of destructive fluorescent that has to be added to cells.
This step can greatly reduce the time-consuming and laborious tissue preparation process and further improve \gls{gl:high-throughput}~\cite{gyuhyun_lee_deephcs_2021}.
Christiansen et al.~\cite{christiansen_silico_2018} have introduced the approach ``In Silico Labeling'' (ISL) using
deep learning to predict fluorescent labeling from transmitted light images.
Building upon this, Lee et al.~\cite{gyuhyun_lee_deephcs_2018,gyuhyun_lee_deephcs_2021} developed DeepHCS and DeepHCS++
to transform brightfield images into multiple fluorescence channels commonly used in high-content screening.
They leveraged multitask learning with adversarial losses to generate realistic virtual stains.

\acrfull{gan} have emerged as a powerful framework for microscopy image generation and translation tasks.
Zhang et al.~\cite{zhang_high-throughput_2019} combined a \gls{gan} with light microscopy to achieve deep learning-based
super-resolution under a large field-of-view.
The model can recover a high-resolution accurate image from its single low-resolution measurement.
In the same year, Scalbert et al.~\cite{marin_scalbert_generic_2019} developed a generic isolated cell image generator by approximating
the shape and texture distributions of cell components using \glspl{gan}.
Furthermore, Liu et al.~\cite{liu_multi-modality_2021} used a \gls{gan} to transfer styles across brightfield and fluorescence modalities
to augment limited annotated data for cell segmentation.
They were able to improve the segmentation accuracy of the two top-ranked Mask \acrfull{rcnn}-based nuclei segmentation algorithms significantly.
And very recently, Liu et al.~\cite{liu_saturation_2024} proposed a two-stage cell image recovery model that is able to
reduce or even remove phenotypic feature loss caused by saturation artifacts, enabling a full analysis of the morphological features of cells in a given microscopy image.

More recently, diffusion models have shown promise for image generation in this domain.
Cross-Zamirski et al.~\cite{cross-zamirski_class-guided_2023} introduced a class-guided diffusion model to generate
cell painting images from brightfield along with class labels.
Taking a different approach, Della Maggiora et al.~\cite{gabriel_della_maggiora_conditional_2023} presented a conditional variational diffusion model
that learns the noise schedule during training itself, trying to improve a common drawback with diffusion models, and
demonstrating applications in super-resolution microscopy and quantitative phase imaging.
Lu et al.~\cite{lu_diffusion-based_2024} developed EMDiffuse, a suite of algorithms designed to enhance Electron Microscopy (EM) and Volume Electron Microscopy (vEM).
It demonstrates proficiency in several tasks such as reconstruction and generation.
Incorporating the physical model of microscopy image formation into the loss function of a \acrfull{ddpm}, Li et al.~\cite{li_microscopy_2023} suggest a physics-informed \gls{ddpm} (PI-DDPM)
to improve reconstructions with reduced artifacts compared to regular \glspl{ddpm} when trained on synthetic microscopy data.

In summary, the development of deep learning methods for artificially generating and improving brightfield images
has made significant progress.
Techniques include unconditional generation, cross-modality translation, super-resolution, and style transfer.
Although previous research relied heavily on \glspl{gan}, diffusion models are gaining popularity due to their ability to generate high-quality and diverse samples.
However, in particular when training data is limited, further research is needed to improve the reliability and utility of the generated images.
The methods discussed here lay the foundations for this research on using unconditional diffusion models to generate
realistic brightfield images to improve single cell detection.

\section{AI-Based Microscopy Image Analysis}\label{sec:ai-based-microscopy-image-analysis}
In recent years, deep learning approaches have achieved state-of-the-art performance on cell segmentation and detection tasks in microscopy images.
\glspl{cnn} have proven particularly effective due to their ability to learn hierarchical features directly from raw pixel data.
One of the most influential deep learning architectures for biomedical image segmentation is U-Net,
originally proposed by Ronneberger et al.~\cite{ronneberger_u-net_2015}.
U-Net uses an encoder-decoder structure with skip connections to combine high-resolution features from the contracting path with upsampled outputs from the expanding path.
This allows the network to produce precise localization while capturing context.
The authors demonstrated U-Net's ability to achieve good segmentation results with very little training data in microscopy applications, while maintaining a very high computation speed.
Falk et al.~\cite{thorsten_falk_u-net_2019} later released a user-friendly ImageJ~\cite{schindelin_fiji_2012} plugin to make U-Net accessible to non-experts for cell segmentation and detection tasks.

Many subsequent works have built upon the U-Net architecture.
Caicedo et al.~\cite{juan_c_caicedo_evaluation_2019} performed an extensive evaluation of U-Net and other deep learning strategies for nucleus segmentation in fluorescence images.
They found that U-Net outperformed classical image processing algorithms and could reduce biologically relevant errors by half.
Raza et al.~\cite{shan_e_ahmed_raza_micro-net_2019} offered a multi-scale version of U-Net called Micro-Net for segmenting various objects in microscopy images.
By training the network at multiple resolutions, Micro-Net improved performance on datasets with high variability in cell sizes.
To further improve research in \gls{gl:brightfield-microscopy}, Salem et al.~\cite{danny_salem_yeastnet_2021} present a U-Net architecture, called YeastNet, to conduct semantic segmentation on \gls{gl:brightfield-microscopy} images.
The presented model is able to accurately generate segmentation masks, which can further be used for cell labeling and tracking.
To evaluate the performance on the HeLa \gls{gl:cell-line}, Ghaznavi et al.~\cite{ali_ghaznavi_cell_2022} proposed a residual attention U-Net and compared it with an attention and simple U-Net.
They came to the conclusion that the residual attention architecture achieved the best results regarding the mean \gls{iou} and Dice metric.
Li et al.~\cite{bo_li_label-free_2023} trained the state-of-the-art object detection model \gls{yolo}X~\cite{ge_yolox_2021} on 2D \gls{gl:brightfield-microscopy} images to detect T-cells with a mean average precision of 96.32\%.
Taking it a step further in 2024, Ferreira et al.~\cite{e_d_ferreira_classification_2024} leverage \gls{cnn}-based models to not only detect cells, but also classify them into different lineages with high accuracy.

Other enhancements to U-Net have also been explored.
Ounkomol et al.~\cite{chawin_ounkomol_label-free_2018} used a label-free approach to predict 3D fluorescence directly from transmitted-light images, which can be extended to predict immunofluorescence from electron micrograph input.
Taking a different approach, Arbelle et al.~\cite{assaf_arbelle_microscopy_2018} drew inspiration from \glspl{gan} to train U-Net in a weakly supervised manner, reducing the need for pixelwise annotations.
By adding a modified encoded branch to the standard U-Net architecture, Long et al.~\cite{long_microscopy_2020} proposed a light-weighted variant, called U-Net+.
This enables the model to work with low-resource computing hardware, while increasing the average \acrfull{iou} by 1--3\% compared to competing methods.

Beyond U-Net, other \gls{cnn} architectures have also been applied to cell segmentation.
3 years before the U-Net architecture, Akram et al.~\cite{saad_ullah_akram_cell_2016} recommend a \gls{cnn}-based method that provides high-quality segmentation candidates, which can be used for cell detection, segmentation and tracking.
These segmentation candidates provide an efficient way of exploiting the spatial and temporal context to aid in the decision process of ambiguous, overlapping regions.
Xie et al.~\cite{weidi_xie_microscopy_2018} leverage convolutional neural networks to regress a cell spatial density map across the image.
This method works especially well, when standard image segmentation fails due to cell clumping or overlap, enabling them to set new state-of-the-art performances
for cell counting and detection.
Pan et al.~\cite{xipeng_pan_cell_2018} designed a novel multi-scale fully convolutional neural network for regression of a density map to robustly detect the nuclei of pathology and microscopy images.
To capture the state-of-the-art in 2020, Waithe et al.~\cite{dominic_waithe_object_2020} benchmarked leading object detection networks like Faster \gls{rcnn}, \gls{yolo}v2, \gls{yolo}v3, and RetinaNet for cell detection in \gls{gl:fluorescence-microscopy}.
And Mohammed et al.~\cite{mohammed_a_s_ali_evaluating_2021} performed an extensive comparison of U-Net with other state-of-the-art segmentation networks like U-Net++, DeepLabv3+, and a novel architecture called PPU-Net across multiple microscopy modalities,
showing, that deeper architectures outperform standard U-Net.

In addition to 2D segmentation, deep learning has been used to analyze 3D volumetric microscopy data.
Lugagne et al.~\cite{jean-baptiste_lugagne_delta_2020} developed DeLTA, a 3D cell segmentation and tracking pipeline based on U-Net.
Their approach enabled high-throughput analysis of E. coli cells in a microfluidic device without human intervention.

Litjens et al.~\cite{geert_litjens_survey_2017} reviewed over 300 contributions to the field of medical image analysis to give a concise overview of tasks and applications.
They end with a summary of the current state-of-the-art, as well as a critical discussion about open challenges and future research.
Building on this work, Moen et al.~\cite{erick_moen_deep_2019} and Durkee et al.~\cite{madeleine_s_durkee_artificial_2021} provide a comprehensive review of how deep learning is transforming all facets of cellular image analysis, including classification, segmentation, and augmented microscopy.
Liu et al.~\cite{zhichao_liu_survey_2021} present a survey on applications of deep learning in microscopy image analysis, and also discuss drawbacks of existing deep learning-based methods, highlighting the acquisition of labeled training data.

In addition, Castiglioni et al.~\cite{isabella_castiglioni_ai_2021} also dive into the topic of unbalanced medical datasets, as well as explainable \gls{ai} to deal with the so-called black box issue~\cite{rai_explainable_2020}.
Kopel et al.~\cite{pavel_kopel_cell_2019} give a comprehensive review on cell segmentation for \gls{gl:label-free} contrast microscopy,
undermining the need of fluorescence-free microscopy analysis, since the step of cell staining is time-consuming and toxic.
The internal structure of the cell is destroyed, rendering the cell unable for later reusability~\cite{bo_li_label-free_2023}.

In summary, deep learning has revolutionized the field of cellular image segmentation and detection.
While challenges remain, such as the need for extensive annotated training data, advances in weakly supervised and \gls{gl:label-free} approaches are promising.
As microscopy continues to generate ever-increasing quantities of data, deep learning will be an indispensable tool for extracting biological insights at scale.

#########################################################
################### chapters/03_background.tex ###################
#########################################################

\chapter{Technical Background}\label{ch:technical-background}

\section{Diffusion Fundamentals}\label{sec:diffusion-fundamentals}
Diffusion models are a class of generative models that learn the underlying distribution of the training data and generate
new samples that are similar to the learned distribution~\cite{ho_denoising_2020,song_denoising_2020}.
These models are inspired by the principles of non-equilibrium thermodynamics~\cite{jascha_sohl-dickstein_deep_2015} and have demonstrated state-of-the-art performance
in generating high-quality images.
The application of diffusion models extends beyond image generation, as they have been successfully employed in tasks
such as audio synthesis~\cite{flavio_schneider_archisound_2023,zhifeng_kong_diffwave_2020,zhifang_guo_audio_2023},
voice cloning~\cite{tasnima_sadekova_unified_2022,vadim_popov_diffusion-based_2021} and lately even video generation~\cite{ho_video_2022, omer_bar-tal_lumiere_2024}.

\subsection{Overview}\label{subsec:overview-diff}

The core mechanism of diffusion models, firstly presented by Sohl-Dickstein et al.~\cite{jascha_sohl-dickstein_deep_2015} and further developed by Ho et al.~\cite{ho_denoising_2020},
involves a two-stage process: the ~\textbf{forward diffusion process} (Fig.~\ref{fig:forward-diffusion-process}) and the \textbf{reverse diffusion process} (Fig.~\ref{fig:backward-diffusion-process}).
During the forward diffusion process, the training data is gradually corrupted by adding Gaussian noise at each step.
This process aims to transform the complex data distribution into a simple, tractable distribution, typically a standard Gaussian distribution.
The reverse diffusion process, on the other hand, learns to recover the original data from the noisy versions by reversing
the noising process step by step.
By training the model to denoise the corrupted data, it effectively learns the underlying structure and patterns present in the training data.

Once trained, diffusion models can generate new data by sampling random Gaussian noise and passing it through the learned denoising process.
The model gradually refines the noise, step by step, to produce a sample that resembles the training data.
This generative process allows diffusion models to create novel and diverse samples that capture the essential features of the learned distribution.

Diffusion models have gained significant attention in the research community due to their ability to address the challenges
commonly associated with adversarial training in \glspl{gan}~\cite{goodfellow_generative_2014}.
These challenges include training instability~\cite{juan_c_caicedo_evaluation_2019}, mode collapse~\cite{ricard_durall_combating_2020,youssef_kossale_mode_2022}, and difficulty in achieving convergence~\cite{samuel_a_barnett_convergence_2018}.
In contrast, diffusion models offer several advantages, such as improved training stability, efficiency, scalability,
and the ability to parallelize the training process~\cite{dhariwal_diffusion_2021}.
These properties make diffusion models an attractive choice for generative modeling tasks, particularly in scenarios where stable and reliable training is crucial.

\subsection{Mathematical Concepts}\label{subsec:mathematical-concepts}
\subsubsection*{Forward Diffusion}
Taking a data point $\mathbf{x}_0 \sim q(\mathbf{x})$ from a real data distribution,
a \textit{forward diffusion process} can be defined in which small amounts of Gaussian noise are added to the sample in $T$ number of steps,
producing a sequence of noise samples $\mathbf{x}_1, \ldots, \mathbf{x}_T$~\cite{ho_denoising_2020}.
The step sizes are controlled by a variance schedule $\{\beta_t \in (0,1)\}_{t=1}^T$.
\begin{equation}
    q(\mathbf{x}_t | \mathbf{x}_{t-1}) \coloneqq \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}) \quad q(\mathbf{x}_{1:T} | \mathbf{x}_0) \coloneqq \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1})
    \label{eq:forward-diffusion-process}
\end{equation}
With increasing steps $t$, the data sample $\mathbf{x}_0$ loses more and more distinguishable features until
$\mathbf{x}_T$ is indifferentiable from an isotropic Gaussian distribution for $T \rightarrow \infty$~\cite{ho_denoising_2020}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/forward_diffusion_process}
        \caption{Slowly adding noise to the data sample $\mathbf{x}_0$ over $T$ steps.}
        \label{fig:forward-diffusion-process}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/reverse_diffusion_process}
        \caption{Gradually removing noise from the data sample $\mathbf{x}_T$ over $T$ steps.}
        \label{fig:backward-diffusion-process}
    \end{subfigure}
    \caption{The forward (a) and backward (b) diffusion process.}
    \label{fig:full-diffusion-process}
\end{figure}

\(\mathbf{I}\) (~\ref{eq:forward-diffusion-process}) denotes the identity matrix indicating that each dimension in this multi-dimension scenario has the same standard deviation \(\beta_t\).
Note that \(q(\mathbf{x}_t|\mathbf{x}_{t-1})\) remains a normal distribution, defined by the mean \(\mu\) and the
variance \(\Sigma\), where \(\mu_t = \sqrt{1 - \beta_t} \mathbf{x}_{t-1}\) and \(\Sigma_t = \beta_t \mathbf{I}\)~\cite{ho_denoising_2020}.
%The matrix \(\Sigma\) will always be a diagonal matrix of variances (here \(\beta_t\)).

Thus, distinguishing diffusion models from other latent variable models, the transition from the input data \(\mathbf{x}_0\) to
\(\mathbf{x}_T\), the approximate posterior $q(\mathbf{x}_{1:T} | \mathbf{x}_0)$, also called \textit{forward process}, can be defined as~\cite{ho_denoising_2020}:
\begin{equation}
    q(\mathbf{x}_{1:T} | \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t | \mathbf{x}_{t-1})
    \label{eq:posterior-probability}
\end{equation}
While this approach is mathematically sound, it poses practical challenges.
For instance, at timestep \(t = 500 < T\), \(q\) needs to be applied 500 times to sample \(\mathbf{x}_t\).
This repetitive application can be computationally expensive.

By defining \(\alpha_t = 1 - \beta_t\), \(\bar{\alpha}_t = \prod_{s=0}^t \alpha_s\),
where \(\epsilon_0, \ldots, \epsilon_{t-2}, \epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})\),
the reparameterization~\cite{diederik_p_kingma_auto-encoding_2014} trick can be recursively used to prove that~\cite{ho_denoising_2020}:
\begin{equation}
    \begin{split}
        \mathbf{x}_t &= \sqrt{1 - \beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t} \epsilon_{t-1}\\
        &= \sqrt{\alpha_t} \mathbf{x}_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-2}\\
        &= \cdots\\
        &= \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_0
    \end{split}
        \label{eq:reparametrization-trick}
\end{equation}
Thus, to produce a sample \(\mathbf{x}_t\), following, simplified distribution can be used~\cite{ho_denoising_2020}:
\begin{equation}
    \mathbf{x}_t \sim q(\mathbf{x}_t|\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t) \mathbf{I})
    \label{eq:final-distribution}
\end{equation}
Since \(\beta_t\) is a hyperparameter, \(\alpha_t\) and \(\bar{\alpha}_t\) can be precomputed for all timesteps,
the diffusion process is able to sample noise at any timestep \(t\) and obtain \(\mathbf{x}_t\) in one step~\cite{ho_denoising_2020}.
Therefore, the latent variable \(\mathbf{x}_t\) can be sampled at any arbitrary timestep, without needing to apply ~\ref{eq:final-distribution} \(t\) times~\cite{ho_denoising_2020}.

Ho et al.~\cite{ho_denoising_2020} proposed to utilise a linear variance schedule for \(\beta_t\) increasing from \(\beta_1 = 10^{-4}\) to \(\beta_T = 0.02\).
These boundaries were chosen to be relatively small compared to the data that was scaled to \([-1, 1]\), ensuring that the reverse and forward processes have approximately
the same functional form while keeping the signal-to-noise ratio at \(x_T\) as small as possible~\cite{ho_denoising_2020}.
Nichole et al.~\cite{nichol_improved_2021} however, showed that a cosine variance scheduler works even better, by providing a smoother degradation
of the image.
Hence, allowing the model to operate longer on less noisy images.

\subsubsection*{Reverse Diffusion}
As \(T \rightarrow \infty\), the latent variable \(\mathbf{x}_T\) approximates an isotropic Gaussian distribution~\cite{ho_denoising_2020}.
Therefore, if the reverse distribution \(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\) can be learned, \(\mathbf{x}_T\) can be sampled
from \(\mathcal{N}(0, \mathbf{I})\)~\cite{ho_denoising_2020}.
By running the reverse process, and obtaining a sample from \(q(\mathbf{x}_0)\),
new, synthetic data points can be generated from the original data distribution~\cite{ho_denoising_2020}.
Unfortunately, \(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\) is unknown.
%This distribution is impractical because statistical estimates of \(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\) requires computations involving the original data distribution~\cite{karagiannakos2022diffusionmodels}.

Instead, \(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\) is approximated using a parameterized model \(p_\theta\): a neural network.
Given that \(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\) will also be Gaussian for sufficiently small \(\beta_t\),
\(p_\theta\) is chosen to be Gaussian and the mean and variance are parameterized as follows~\cite{ho_denoising_2020,jascha_sohl-dickstein_deep_2015}:
\begin{equation}
    p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))
    \label{eq:reverse-diffusion-process}
\end{equation}
To go from \(\mathbf{x}_T\) to the data distribution, the reverse formula for all timesteps (\(p_\theta(\mathbf{x}_{0:T})\), also called \textit{reverse process}) can be applied~\cite{ho_denoising_2020}:
\begin{equation}
    p_\theta(\mathbf{x}_{0:T}) = p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)
    \label{eq:reverse-diffusion-process2}
\end{equation}
%To predict the Gaussian parameters (\(\mu_\theta(\mathbf{x}_t, t)\) and \(\Sigma_\theta(\mathbf{x}_t, t)\)) for each timestep,
%the model can be conditioned on timestep \(t\)~\cite{karagiannakos2022diffusionmodels}.

\subsection{Model Architectures}\label{subsec:model-architectures}
Two common backbone architectures are generally chosen for diffusion models: U-Net and Transformer.

\subsubsection*{U-Net}
The U-Net, as already mentioned in chapter~\ref{sec:ai-based-microscopy-image-analysis}, is a convolutional neural network architecture that was originally developed for biomedical image segmentation
by Ronneberger et al.~\cite{ronneberger_u-net_2015} in 2015.
The defining characteristic of the U-Net architecture is its symmetric U-shaped structure, consisting of a contracting path (encoder) and an expansive path (decoder)~\cite{ronneberger_u-net_2015}.
The contracting path follows the typical design of a convolutional network, applying a series of convolutions and max pooling operations to capture hierarchical features.
Whereas, the expansive path increases the spatial resolution~\cite{ronneberger_u-net_2015}.
Each step in the expansive path consists of upsampling the feature map, followed by a convolution.
A key innovation is the use of skip connections, which concatenate features from the contracting path with the upsampled features in the expansive path.
This allows the network to combine high-resolution spatial information from the contracting path with the semantic information learned in the expansive path~\cite{ronneberger_u-net_2015}.

In diffusion models, the U-Net is used to predict the noise that was added to the image at each timestep of the diffusion process~\cite{dhariwal_diffusion_2021}.
The skip connections in the U-Net, specifically, allow for the preservation of fine details and spatial information during the denoising process~\cite{dhariwal_diffusion_2021}.
Several modifications and extensions have been made to the original U-Net architecture for its application in diffusion models.
These include the use of residual blocks, attention mechanisms, and the incorporation of timestep embeddings to condition the network on the noise level~\cite{ho_denoising_2020,song_denoising_2020,nichol_glide_2021}.

\subsubsection*{Transformer}
The Transformer is a deep learning architecture that has revolutionized natural language processing and is now being
applied to other domains like computer vision.
It was first proposed in the seminal 2017 paper ``Attention Is All You Need'' by Vaswani et al.~\cite{ashish_vaswani_attention_2017}.
The key innovation of the Transformer is its use of self-attention mechanisms instead of recurrent or convolutional layers to process sequential data.
The main components of the Transformer architecture are~\cite{ashish_vaswani_attention_2017}:
\begin{enumerate}
    \item Input embedding layer: Converts input tokens into dense vector representations.
    Learned embeddings capture semantic and syntactic properties of the tokens.
    \item Positional encoding: Injects information about the relative or absolute position of tokens in the sequence, since the Transformer contains no recurrence.
    Positional encodings allow the model to make use of the order of the sequence.
    \item Multi-head self-attention: Allows the model to jointly attend to information from different representation subspaces at different positions.
    Multiple attention heads learn different relationships between tokens in the sequence.
    \item Feed-forward networks: Consist of two linear transformations with a ReLU activation in between.
    These are applied to each position separately and identically to transform the attended representations.
    \item Layer normalization and residual connections: Used after each sub-layer to stabilize training.
    Residual connections help propagate the signal and layer normalization keeps the scale of representations consistent across layers.
\end{enumerate}
The Transformer also follows an encoder-decoder structure, with the encoder mapping the input sequence to a sequence of continuous representations,
and the decoder generating an output sequence one element at a time while attending to the encoder representations~\cite{ashish_vaswani_attention_2017}.

Only recently, the Transformer architecture has been adapted for use in diffusion models for image generation.
Peebles et al.~\cite{william_s_peebles_scalable_2022} proposed the \acrfull{dit}, which replaces the typical U-Net backbone of diffusion models with a Transformer that operates on latent image patches.
The \gls{dit} architecture makes a few key modifications to the standard Transformer~\cite{william_s_peebles_scalable_2022}:
\begin{itemize}
    \item It uses an adaptive layer normalization mechanism to inject conditional inputs like the diffusion timestep and class label into each Transformer block.
    \item The input ``patchify'' layer linearly embeds image patches into a sequence of tokens that are fed into the Transformer stack.
    \item Deeper and wider \gls{dit} variants with more Transformer blocks and attention heads tend to improve performance, but also increase computation time and cost.
\end{itemize}
The largest \gls{dit}-XL/2 model outperformed prior diffusion models on the class-conditional ImageNet generation benchmarks while being relatively compute-efficient~\cite{william_s_peebles_scalable_2022}.

\subsection{Applications and Popular Models}\label{subsec:applications-and-latest-trends}
Diffusion models have found applications in a wide range of areas, leveraging their ability to model complex data distributions and generate realistic samples.
Some notable applications include:
\begin{itemize}
    \item Image Generation: Diffusion models excel at generating high-resolution, diverse, and realistic images.
    Models like DALL-E 2~\cite{openai_dalle_2}, Imagen~\cite{saharia_photorealistic_2022}, and Stable Diffusion~\cite{rombach_high-resolution_2022,patrick_esser_scaling_2024} have showcased impressive capabilities in text-to-image synthesis,
    enabling users to generate images based on textual descriptions.
    \item Image Inpainting and Outpainting: Diffusion models can be used for image inpainting~\cite{andreas_lugmayr_repaint_2022,c_corneanu_latentpaint_2024}, where missing regions of an
    image are filled in, and outpainting~\cite{shaofeng_zhang_continuous-multiple_2024}, where the image is extended beyond its original boundaries.
    This has applications in image editing, restoration, and creative design.
    \item Super-Resolution: Diffusion models can enhance the resolution and quality of low-resolution images,
    enabling the generation of high-resolution versions while preserving important details and structures~\cite{brian_b_moser_diffusion_2024,yufei_wang_sinsr_2023,zongsheng_yue_resshift_2023}.
    \item Video Generation: Diffusion models have been extended to generate videos by modeling the temporal dynamics and generating frames sequentially.
    This has potential applications in video synthesis, animation, and special effects~\cite{ho_video_2022,omer_bar-tal_lumiere_2024,ho_imagen_2022}.
    \item Audio Generation: Diffusion models can be applied to audio data, enabling the generation of realistic speech, music, and sound effects.
    This has implications for text-to-speech systems, audio synthesis, and creative applications~\cite{flavio_schneider_archisound_2023,rongjie_huang_make--audio_2023}.
\end{itemize}

Several diffusion models have gained significant attention and popularity due to their impressive performance and wide-ranging capabilities.
Stable Diffusion, developed by Stability \gls{ai}, is an open-source text-to-image diffusion model that has gained widespread adoption.
It offers high-quality image generation capabilities and supports various styles and domains.
With version one~\cite{rombach_high-resolution_2022}, two~\cite{rombach_high-resolution_2022} and three~\cite{patrick_esser_scaling_2024},
Stable Diffusion has been a leader in advancing the art and technology of image generation using diffusion models.
Developed by OpenAI, DALL-E 2~\cite{openai_dalle_2} \& 3~\cite{openai_dalle_3} are powerful text-to-image diffusion models that can generate highly realistic and diverse images from textual descriptions.
They have showcased remarkable abilities in capturing complex scenes and concepts.
Imagen~\cite{saharia_photorealistic_2022} was developed by Google and is another state-of-the-art text-to-image diffusion model that produces high-quality images with fine-grained control over the generated content.
Lastly, Midjourney~\cite{midjourney} is a popular diffusion-based model that focuses on artistic and creative image generation.
It allows users to explore and generate images with unique styles and aesthetics

\subsection{Unconditional Diffusion}\label{subsec:unconditional-diffusion}
While the previous sections have provided an overview of diffusion models and their applications,
it is important to distinguish between conditional and unconditional diffusion models,
as this study focuses specifically on unconditional diffusion-based image generation.
Unconditional diffusion models generate samples from a learned data distribution without any additional input or conditioning information.
In contrast, conditional diffusion models incorporate extra information to guide the generation process, such as class labels, text prompts, or other forms of conditioning.

The key difference lies in the input and training process.
Unconditional models are trained solely on a dataset of images, learning to generate samples that match the overall
distribution of the training data.
Whereas, conditional models receive additional inputs that allow more controlled generation, such as generating images
based on a text description or belonging to a specific class.
For unconditional image generation, the diffusion model learns to denoise randomly sampled noise tensors into coherent
images resembling the training distribution, without any guiding information.
This allows for open-ended image synthesis but provides less control over the specific content generated.
Some key advantages of this approach include simplicity, flexibility and unsupervised learning.
Unconditional diffusion models do not require paired data or labels, simplifying data collection and training.
They can generate diverse samples from the learned distribution without constraints, while also potentially discovering
underlying patterns and structures in the data distribution without supervision.
However, unconditional diffusion models lack control over the generated content, compared to their conditional counterparts.
There is no direct way to specify desired attributes or content in the generated images.
This characteristic intrinsically links a single unconditional diffusion model to a specific dataset representing a particular biological setup.
When dealing with multiple \glspl{gl:cell-line}, experimental conditions, or imaging setups, it becomes necessary to train separate models for each scenario.
This represents a limitation of unconditional diffusion models, as it necessitates more extensive and multiple training processes.
In contrast, conditional models offer the advantage of being trainable on a single, diverse dataset,
where each image is labeled with its corresponding condition.
This allows for greater flexibility and efficiency in handling varied experimental setups within a single model framework.

In the context of this study, unconditional diffusion models present an interesting avenue for generating diverse microscopy images.
By learning the underlying distribution of cellular images without explicit conditioning,
these models may capture nuanced features and variations that could impact detection algorithms.
The use of unconditional models also aligns with the often label-scarce nature of microscopy datasets,
where paired condition-image data may be limited.

\section{Object Detection Fundamentals}\label{sec:object-detection-fundamentals}
Object detection is a fundamental task in computer vision that involves identifying and localizing objects of interest within an image or video frame~\cite{zhengxia_zou_object_2019}.
The goal is to develop computational models that can accurately determine what objects are present and where they are located in the visual input.
This information is crucial for a wide range of applications, from self-driving cars and video surveillance to medical imaging and robotics~\cite{vahab2019applications}.

\subsection{Overview}\label{subsec:overview-od}
The importance of object detection lies in its ability to provide a foundation for higher-level computer vision tasks.
By accurately detecting and localizing objects, subsequent processes such as object tracking, instance segmentation,
and scene understanding can be performed more effectively~\cite{zhong-qiu_zhao_object_2019,ayoub_benali_amjoud_object_2023}.
Object detection enables computer vision systems to interpret and interact with their environment,
making it a critical component in the development of intelligent and autonomous systems~\cite{vahab2019applications}.

Object detection techniques have evolved significantly in the last two decades by going through two main historical periods~\cite{zhengxia_zou_object_2019}:
traditional object detection (pre-2014) and deep learning-based object detection (post-2014).
The early approaches relied on template matching, sliding window methods and many more classical image processing techniques.
These methods were limited in their ability to handle variations in object appearance, scale, and occlusion.
The introduction of AlexNet in 2012~\cite{alex_krizhevsky_imagenet_2012}, originally designed for image recognition and classification,
started the revolution of object detection to rely heavily on deep learning, especially the \gls{cnn}-based model architecture.
It showed that \glspl{cnn} could automatically learn powerful visual features, reducing the need for hand-crafted features in object detection pipelines~\cite{alex_krizhevsky_imagenet_2012}.

In general, there are the two main approaches for detecting objects in images using deep learning: \textbf{One-stage object detectors} and \textbf{two-stage object detectors}.
The key difference is that two-stage detectors like Faster \gls{rcnn}~\cite{shouxin_ren_faster_2017} first generate region proposals using a \acrfull{rpn},
and then classify and refine the coordinates of these proposals in a second stage.
In contrast, one-stage detectors like \gls{yolo}X~\cite{Redmon2015YouOL} and \gls{ssd}~\cite{Liu2015SSDSS} directly predict object classes and locations in a single forward pass of the network,
skipping the region proposal step.
This makes one-stage detectors faster and simpler, but sometimes less accurate than two-stage methods.

Within these two approaches, models can further be categorized as anchor-based or anchor-free (Fig.~\ref{fig:anchorbased-vs-anchorfree}).
Anchor-based detectors like Faster \gls{rcnn}~\cite{shouxin_ren_faster_2017} and RetinaNet~\cite{lin_focal_2017} use preset anchor boxes of different scales and aspect ratios,
and predict offsets relative to these anchors.
Anchor-free detectors like CornerNet~\cite{Law2018CornerNetDO} and some versions of \gls{yolo}~\cite{ge_yolox_2021} eliminate anchor boxes, and instead directly predict keypoints like
object centers or corners.
The advantage is they avoid the complexities of tuning anchor hyperparameters.
Overall, while two-stage anchor-based detectors dominated for many years, one-stage anchor-free models have rapidly caught
up in accuracy while being faster and more flexible.
\begin{figure}[h!]
    \includegraphics[width=\textwidth]{images/anchorbased_anchorfree}
    \caption{The left image of the figure illustrates the anchor-based approach, where multiple bounding boxes of varying sizes and
        aspect ratios are generated around the object, represented by different colored rectangles.
        The right image shows the anchor-free approach, where the object is directly detected without predefined anchor boxes,
        focusing on the center and scale of the object, indicated by red arrows. This visual comparison highlights the
        difference in methodology between anchor-based and anchor-free object detectors.
        (Image source: Zhang et al.~\cite{jianming_zhang_siamese_2021})}
    \label{fig:anchorbased-vs-anchorfree}
\end{figure}

\subsection{History of Object Detectors}\label{subsec:history-of-object-detectors}
The early era of object detection was characterized by handcrafted features and ingenious designs to overcome the
limitations of computing power and image representation.
In 2001, Viola and Jones~\cite{Viola2001RapidOD,Viola2001RobustRF} achieved a breakthrough with their real-time face detection algorithm.
Running on modest hardware, their detector was remarkably faster than contemporary methods while maintaining comparable accuracy.
The Viola-Jones detector introduced three key innovations: \textit{Integral image representation}, \textit{Feature selection} and \textit{Detection cascades}.
These techniques allowed for efficient computation and rapid rejection of non-face regions, enabling real-time performance.
Dalal and Triggs~\cite{Dalal2005HistogramsOO} proposed the \acrfull{hog} descriptor in 2005, which represented a significant
improvement over previous feature extraction methods.
\gls{hog} balanced feature invariance and nonlinearity by computing gradients on a dense grid of uniformly spaced cells with
overlapping local contrast normalization.
While versatile, \gls{hog} was primarily motivated by pedestrian detection.
\acrfull{dpm}, developed by Felzenszwalb et al.~\cite{Felzenszwalb2008ADT,Felzenszwalb2010CascadeOD,Felzenszwalb2010ObjectDW},
dominated object detection challenges from 2008 to 2010.
It extended the \gls{hog} detector using a ``divide and conquer'' philosophy, decomposing objects into parts for more robust detection.
\gls{dpm} introduced several influential concepts: \textit{Mixture models}, \textit{Hard negative mining}, \textit{Bounding box regression} and \textit{Context priming}.
Although surpassed in accuracy by modern methods, many of \gls{dpm} 's insights, like \textit{Bounding box regression} continue to influence contemporary object detectors.
Going into more detail on these methods is beyond the scope of this study, but they laid the foundation for modern object detection techniques.

The rise of deep learning, particularly \glspl{cnn}, revolutionized object detection~\cite{krizhevsky2012imagenet}.
This era can be broadly categorized into two-stage and one-stage detectors.
Girshick et al.~\cite{ross_girshick_rich_2014,Girshick2016RegionBasedCN} introduced Regions with \gls{cnn} features (\gls{rcnn}) in 2014,
marking the beginning of the deep learning era in object detection.
\gls{rcnn} used selective search to generate object proposals, then applied a CNN to extract features from each proposal.
Linear SVMs were used for final classification.
While \gls{rcnn} significantly improved detection accuracy, it suffered from slow inference due to redundant computations.
He et al.~\cite{He2014SpatialPP} proposed \acrfull{sppnet} in 2014 to address \gls{rcnn}'s speed limitations.
\gls{sppnet} introduced a Spatial Pyramid Pooling (SPP) layer, allowing \glspl{cnn} to generate fixed-length representations regardless of input image size.
This innovation enabled to compute the features of an entire image just once, dramatically improving detection speed.
Building on his previous work, Girshick~\cite{ross_girshick_fast_2015} proposed Fast \gls{rcnn} in 2015.
This detector enabled simultaneous training of a detector and bounding box regressor under the same network configuration.
Fast \gls{rcnn} significantly improved both accuracy and speed compared to its predecessors.
Ren et al.~\cite{shouxin_ren_faster_2017} introduced Faster \gls{rcnn} shortly after Fast \gls{rcnn}~\cite{ross_girshick_fast_2015}, achieving near real-time detection speeds.
The key innovation was the \gls{rpn}, which enabled efficient generation of object proposals within the neural network.
Faster \gls{rcnn} represented a major step towards end-to-end trainable detection systems.
Lin et al.~\cite{Lin2016FeaturePN} proposed \acrfull{fpn} in 2017 to address the challenge of detecting objects at various scales.
\gls{fpn} introduced a top-down architecture with lateral connections, building high-level semantic feature maps at all scales.
This approach has significantly improved detection of objects across a wide range of sizes.

\acrfull{yolo}, proposed by Joseph et al.~\cite{Redmon2015YouOL} in 2015, was the first one-stage detector of the deep learning era.
\gls{yolo} framed detection as a regression problem, applying a single neural network to the full image to predict bounding boxes and class probabilities simultaneously.
While sacrificing some localization accuracy, especially for small objects, \gls{yolo} achieved unseen detection speeds.
Since then, many more versions of \gls{yolo} have been developed and published~\cite{Redmon2016YOLO9000BF,Redmon2018YOLOv3AI,Bochkovskiy2020YOLOv4OS,ge_yolox_2021,Wang2022YOLOv7TB,Wang2024YOLOv9LW,Wang2024YOLOv10RE}.
Liu et al.~\cite{Liu2015SSDSS} introduced the \acrfull{ssd} model in 2015, incorporating multi-reference and multi-resolution detection techniques.
\gls{ssd} improved upon \gls{yolo}v1's accuracy, particularly for small objects, while maintaining high detection speeds.
A key innovation was detecting objects at different scales on different layers of the network.
Lin et al.~\cite{lin_focal_2017} proposed RetinaNet in 2017 to address the accuracy gap between one-stage and two-stage detectors.
They identified extreme foreground-background class imbalance as a key issue and introduced the focal loss to address this problem.
RetinaNet achieved comparable accuracy to two-stage detectors while maintaining the speed advantages of one-stage approaches.

Most of the above-mentioned deep learning-based object detection models rely heavily on non-maximum suppression as a post-processing step.
\acrfull{nms} is a crucial technique used to eliminate redundant bounding
boxes and select the most relevant detections~\cite{jan_hosang_learning_2017}.
The primary purpose of \gls{nms} is to refine the output of object detectors by removing duplicate detections of the same object,
thereby improving the overall accuracy and reducing false positives~\cite{navaneeth_bodla_soft-nms_2017}.
\gls{nms} has its roots in early computer vision techniques.
It was initially proposed as a method for edge detection in images~\cite{canny_computational_1986}.
As object detection algorithms evolved, \gls{nms} was adapted to handle bounding box refinement.
The concept gained significant attention with the rise of sliding window detectors
and has since become an integral part of modern object detection pipelines~\cite{Dalal2005HistogramsOO}.
The \gls{nms} algorithm~\cite{alexander_neubeck_efficient_2006} typically follows the steps shown in Algorithm~\ref{alg:nms}.
\begin{algorithm}
    \caption{Non-Maximum Suppression (NMS)}
    \label{alg:nms}
    \begin{algorithmic}[1]
        \State \textbf{Input:} \\
        \hspace{\algorithmicindent}List of Bounding Boxes $B$ \\
        \hspace{\algorithmicindent}List of Corresponding Scores $S$ \\
        \hspace{\algorithmicindent}$\text{IoU}_\text{threshold}$
        \State \textbf{Output:} List of Remaining Boxes $D$
        \State $D \gets \emptyset$
        \State Sort $B$ in descending order of scores $S$

        \While{$B \neq \emptyset$}
            \State $b_\text{max} \gets$ first box in $B$ \Comment{Get box with highest score}
            \State $D \gets D \cup \{b_\text{max}\}$ \Comment{Add it to the final list}
            \State $B \gets B \setminus \{b_\text{max}\}$ \Comment{Remove it from the initial list}
            \ForAll{$b \in B$}
                \If{$\text{IoU}(b_\text{max}, b) > \text{IoU}_\text{threshold}$}
                    \State $B \gets B \setminus \{b\}$ \Comment{Eliminate boxes with high enough \gls{iou}}
                \EndIf
            \EndFor
        \EndWhile
        \State \Return $D$
    \end{algorithmic}
\end{algorithm}

This process ensures that only the most confident and least overlapping detections are retained,
effectively suppressing less confident, redundant predictions~\cite{Liu2015SSDSS}.
In two-stage detectors like \gls{rcnn} and its variants (Fast \gls{rcnn}, Faster \gls{rcnn}), \gls{nms} is typically applied twice:
First after the \gls{rpn} to refine the proposed regions
and after the final classification and bounding box regression to eliminate duplicate detections~\cite{shouxin_ren_faster_2017}.
For one-stage detectors like \gls{yolo} and \gls{ssd}, \gls{nms} is applied once at the end of the network:
After the network generates a set of bounding boxes and class probabilities in a single forward pass~\cite{Redmon2015YouOL}.
In both cases, \gls{nms} plays a critical role in reducing the number of detections and improving the overall accuracy of the object detection system~\cite{lin_focal_2017}.
But it suffers from several limitations that researchers are trying to address:
\begin{itemize}
    \item Inability to detect nearby objects: Traditional \gls{nms} suppresses all bounding boxes that have significant overlap with the highest-scoring detection.
    This can lead to missed detections when objects are close together or partially occluded~\cite{rasmus_rothe_non-maximum_2014}.
    \item Reliance on arbitrary thresholds: \gls{nms} typically uses a fixed \gls{iou} threshold to determine which boxes to suppress.
    This arbitrary threshold may not be optimal for all object classes and scenarios~\cite{yanan_song_improved_2019,ahmed_husham_al-badri_adaptive_2023}.
    \item Discarding potentially useful information: By completely eliminating overlapping boxes, \gls{nms} discards information that could be valuable for improving detection accuracy~\cite{navaneeth_bodla_soft-nms_2017}.
    \item Sensitivity to localization errors: Small errors in bounding box coordinates can lead to incorrect suppression decisions, especially with high \gls{iou} thresholds~\cite{johannes_gilg_we_2023}.
    \item Computational inefficiency: The iterative nature of \gls{nms} can be computationally expensive, particularly for large numbers of detections~\cite{andrew_shepley_confluence_2020}.
\end{itemize}

The success of Transformer architectures in natural language processing has recently influenced object detection.
In 2020, Carion et al.~\cite{Carion2020EndtoEndOD} proposed \gls{detr} (\textbf{DE}tection \textbf{TR}ansformer), framing object detection as a set prediction problem using Transformers.
This approach eliminated the need for many hand-designed components like anchor generation and most importantly, non-maximum suppression.
Zhu et al.~\cite{Zhu2020DeformableDD} further improved upon this concept with Deformable \gls{detr}, addressing issues of slow convergence and limited performance on small objects.
In 2023, Zhao et al.~\cite{Lv2023DETRsBY} proposed \acrfull{rtdetr}, by building on top of \gls{detr}~\cite{Carion2020EndtoEndOD}.
This approach has enabled transformer-based detection models to be on par when it comes to detection speed of non-transformer models, while
maintaining the accuracy and detection rate of previous \gls{detr} models.
These Transformer-based approaches represent a new paradigm in object detection, achieving state-of-the-art performance on benchmark datasets~\cite{Lv2023DETRsBY}.

\subsection{Datasets and Performance Metrics}\label{subsec:datasets-and-performance-metrics}
The creation of large-scale, diverse datasets has been instrumental in pushing the boundaries of object detection capabilities.
Several key datasets have emerged as benchmarks for the computer vision community, each contributing to the field's progression in unique ways.

The \acrfull{pascal} \acrfull{voc} Challenges, running from 2005 to 2012, were pivotal in the early development of object detection algorithms~\cite{mark_everingham_pascal_2010,mark_everingham_pascal_2015}.
Two versions of this dataset are particularly noteworthy: \textbf{VOC07} and \textbf{VOC12}.
VOC07 consists of 5,000 training images with roughly 12,000 annotated objects,
whereas VOC12 expands this dataset with an additional 6,000 training images and 15,000 annotated objects, resulting in
11,000 images and 27,000 objects.
Both datasets feature annotations for 20 common object classes, such as ``person'', ``cat'' and ``bicycle'', providing a foundation for detecting everyday objects~\cite{mark_everingham_pascal_2010,mark_everingham_pascal_2015}.
\acrfull{ilsvrc}~\cite{olga_russakovsky_imagenet_2015}, organized annually from 2010 to 2017,
significantly advanced the state of the art in generic object detection.
The detection challenge within \gls{ilsvrc} utilized ImageNet images and expanded the scope to 200 object classes.
This dataset marked a substantial increase in scale, with the number of images and object instances surpassing that of \gls{pascal} \gls{voc} by two orders of magnitude.
Introduced in 2014, \acrfull{mscoco} quickly became one of the most challenging and widely used object detection datasets~\cite{tsung-yi_lin_microsoft_2014}.
Key features of \gls{mscoco} include:
\begin{itemize}
    \item A focus on 80 object categories, with fewer classes than \gls{ilsvrc} but more instances per class.
    \item The inclusion of per-instance segmentation annotations, in addition to bounding boxes, to aid in precise localization.
    \item A higher proportion of small objects (area < 1\% of the image) and more densely located objects.
\end{itemize}
The \gls{mscoco}-17 version, for example, contains 164,000 images with 897,000 annotated objects.
The dataset's emphasis on small and densely packed objects has made it particularly valuable for developing robust detection algorithms.
Launched in 2018, the \acrfull{oid} challenge represents the next leap in dataset scale and complexity~\cite{alina_kuznetsova_open_2020}.
The \gls{oid} dataset includes 1.91 million images with 15.44 million annotated bounding boxes across 600 object categories for standard object detection,
and a visual relationship detection task, focusing on identifying paired objects in specific relations.
The unprecedented scale of the \gls{oid} has further pushed the boundaries of object detection research.

The evolution of evaluation metrics in object detection reflects the changing priorities and capabilities of detection algorithms over time.
In the early days of object detection research, particularly in pedestrian detection, metrics such as ``miss rate vs. false positives per window (FPPW)''
were common~\cite{Dalal2005HistogramsOO}.
However, this per-window measurement was found to be flawed and not indicative of full image performance~\cite{piotr_dollar_pedestrian_2009}.
The introduction of the Caltech pedestrian detection benchmark in 2009~\cite{piotr_dollar_pedestrian_2009,piotr_dollar_pedestrian_2012} shifted the focus to false positives per image (FPPI),
providing a more realistic assessment of detector performance in full images.
Currently, the most widely used evaluation metric for object detection is \acrfull{ap}, first introduced in VOC2007~\cite{mark_everingham_pascal_2010}.
\gls{ap} is calculated as the average detection precision across different recall levels and is typically computed for each object category separately.
The \acrfull{map} across all categories serves as an overall performance metric.
To assess localization accuracy, the \gls{iou} between predicted and ground truth bounding boxes is used.
Traditionally, a threshold of 0.5 \gls{iou} has been used to determine whether an object is correctly detected:
\begin{equation}
    \text{mAP@50} = \frac{1}{N} \sum_{i=1}^{N} AP_i
    \label{eq:map50-equation}
\end{equation}

\noindent where:
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item $N$ is the number of classes.
    \item $AP_i$ is the Average Precision for class $i$.
\end{itemize}

The Average Precision (AP) for a single class is computed as:

\begin{equation}
    AP = \sum_{k=1}^{n} (P(k) \cdot \Delta R(k))
    \label{eq:ap-equation}
\end{equation}

\noindent where:
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item $n$ is the number of detections.
    \item $P(k)$ is the precision at cutoff $k$.
    \item $\Delta R(k)$ is the change in recall at cutoff $k$.
\end{itemize}
With the introduction of the \gls{mscoco} dataset~\cite{tsung-yi_lin_microsoft_2014}, a more strict evaluation protocol was established.
The \gls{mscoco} AP is averaged over multiple \gls{iou} thresholds ranging from 0.5 to 0.95, encouraging more precise object localization (~\ref{eq:map50_95-equation}).
This metric has become particularly relevant for applications requiring high localization accuracy, such as robotic manipulation tasks or medical image analysis.
\begin{equation}
    \text{mAP@[50:95]} = \frac{1}{M} \sum_{j=1}^{M} \left( \frac{1}{N} \sum_{i=1}^{N} AP_i(IoU_j) \right)
    \label{eq:map50_95-equation}
\end{equation}

\noindent where:
\setlist{nolistsep}
\begin{itemize}[noitemsep]
    \item $M$ is the number of \gls{iou} thresholds (10, corresponding to 50\%, 55\%, \ldots, 95\%).
    \item $IoU_j$ is the j-th IoU threshold.
    \item $N$ is the number of classes.
    \item $AP_i(IoU_j)$ is the Average Precision for class $i$ at IoU threshold $IoU_j$.
\end{itemize}

The development of comprehensive datasets and rigorous evaluation metrics has been crucial in advancing the field of object detection.
From the early days of \gls{pascal} \gls{voc} to the current state-of-the-art \gls{mscoco} and \gls{oid},
researchers have continually raised the bar for detection algorithms.
Similarly, the evolution of evaluation metrics from simple per-window measurements to sophisticated multi-threshold \gls{ap}
calculations reflects the increasing demands placed on modern object detectors.
As the field continues to progress, it is likely that even more challenging datasets and nuanced evaluation metrics will emerge,
further driving innovation in object detection techniques.

#########################################################
################### chapters/04_motivation.tex ###################
#########################################################

\chapter{Motivation}\label{ch:motivation}
Single-cell analysis has emerged as a powerful approach for understanding cellular heterogeneity and dynamics in biological systems.
By examining individual cells rather than bulk populations, researchers can uncover critical insights into developmental processes,
disease mechanisms, and therapeutic responses~\cite{f_mualla_automatic_2016,e_d_ferreira_classification_2024}.
However, the detection and segmentation of individual cells from microscopy images remains a significant challenge,
particularly when using \gls{gl:label-free} brightfield imaging techniques~\cite{huixia_ren_cellbow_2020,sandhya_prabhakaran_addressing_2023}.
In biological applications, “\gls{gl:label-free}” techniques refer to methods that analyze biological samples without the use of
fluorescent dyes or other external labels, enabling the study of cells and biomolecules in their natural state.
In contrast, ``labels'' in machine learning denote annotated data points used to train supervised models,
providing the necessary output categories or values that enable the model to learn and make accurate predictions.

\section{Current Challenges and Limitations}\label{sec:current-challenges-and-limitations}
Detecting single cells in \gls{gl:brightfield-microscopy} images presents several significant challenges.
One of the primary difficulties is the low contrast between cells and the background,
making it hard to distinguish cell boundaries accurately~\cite{f_mualla_automatic_2016}.
Unlike \gls{gl:fluorescence-microscopy}, where specific cellular components can be labeled, brightfield images often show cells as
transparent objects with minimal intensity differences~\cite{jyrki_selinummi_bright_2009}.
This low contrast is further complicated by illumination variations across the field of view and over time during long-term experiments~\cite{f_mualla_automatic_2016}.
Additionally, cells in brightfield images can exhibit heterogeneous intensity levels, even within the same cell population~\cite{f_mualla_automatic_2016,felix_buggenthin_automatic_2013}.

To combat this issue, fluorescent dyes are often used to lift cells out of the background.
But relying on fluorescence labeling techniques poses its own set of challenges.
While \gls{gl:fluorescence-microscopy} provides high contrast images that clearly delineate cell boundaries, the labeling process
can be time-consuming, expensive, and potentially cytotoxic~\cite{leonor_morgado_rise_2024,e_d_ferreira_classification_2024}.
This limits the ability to perform live-cell imaging over extended time periods.

Another challenge is the presence of imaging artifacts, such as uneven illumination, out-of-focus effects, air bubbles, and debris,
which can be mistaken for cells or obscure actual cell boundaries~\cite{sandhya_prabhakaran_addressing_2023}.
The three-dimensional nature of cells also poses problems when projecting them onto a two-dimensional image plane,
leading to overlapping cell boundaries and apparent ``lateral spillover'' effects~\cite{sandhya_prabhakaran_addressing_2023}.
This is particularly problematic in dense cell cultures or tissues where individual cells are in close proximity~\cite{e_d_ferreira_classification_2024}.

Furthermore, the morphological diversity of different cell types adds another layer of complexity.
Cells can vary greatly in size, shape, and texture, making it challenging to develop a universal detection algorithm~\cite{jean-baptiste_lugagne_identification_2018}.
This diversity necessitates the development of adaptive and robust methods that can handle various cell morphologies without
extensive parameter tuning~\cite{f_mualla_automatic_2016,kaisa_liimatainen_iterative_2019}.
Lastly, the computational demands of processing large datasets from high-throughput microscopy experiments require
algorithms that are not only accurate but also efficient in terms of processing time~\cite{f_mualla_automatic_2016,felix_buggenthin_automatic_2013}.
Modern microscopy experiments can generate vast amounts of image data, necessitating efficient and
scalable computational methods for single-cell detection and analysis~\cite{philipp_angerer_single_2017}.

One of the biggest limitations researchers face is the acquisition of large, high-quality and diverse microscopy datasets:
\begin{enumerate}
    \item Time constraints: Collecting extensive \gls{gl:brightfield-microscopy} data is highly time-consuming.
    Imaging large numbers of samples across multiple conditions and time points can take days or even weeks of continuous microscope operation~\cite{stavroula_skylaki_challenges_2016}.
    This is especially challenging for live-cell imaging experiments that require frequent image acquisition over extended periods.
    \item Cost considerations: High-throughput \gls{gl:brightfield-microscopy} setups are expensive, often requiring specialized
    equipment like automated stages, environmental control systems, and high-performance cameras~\cite{rainer_pepperkok_high-throughput_2006}.
    Additionally, there are ongoing costs for sample preparation materials, microscope maintenance, and data storage infrastructure.
    \item Ethical considerations: When working with biological samples, especially those derived from human subjects or animal models,
    researchers must navigate complex ethical requirements.
    This includes obtaining proper informed consent, ensuring confidentiality of sensitive information, and adhering to
    regulations on the use and storage of biological materials~\cite{anne_cambonthomsen_series_2007,i_galende_ethical_2023,gregory_pappas_exploring_2005}.
    For human samples, researchers must often obtain approval from institutional review boards, which can be a lengthy process\footnote{Panel on Research Ethics. TCPS 2: CORE-2022 (Course on Research Ethics) Chapter 12. Retrieved from: \href{http://tcps2core.ca/welcome}{http://tcps2core.ca/welcome} (Accessed September 09th 2024)}.
    \item Laboratory labor requirements: Preparing samples for large-scale \gls{gl:brightfield-microscopy} experiments is labor-intensive.
    This includes cell culture maintenance, sample mounting, and quality control checks.
    Additionally, operating microscopes for extended periods and managing the resulting data requires significant personnel time and expertise~\cite{mojca_mattiazzi_usaj_high-content_2016,s_asha_saliency_2023}.
    \item Data management challenges: The sheer volume of data generated by large-scale \gls{gl:brightfield-microscopy} experiments
    poses significant storage and analysis challenges.
    Researchers must implement robust data management systems to organize, store, and process terabytes of image data~\cite{jason_r_swedlow_informatics_2003}.
    \item Standardization issues: Ensuring consistency across large datasets can be difficult, especially when experiments
    span multiple days or use different microscope setups.
    Variations in sample preparation, imaging conditions, microscope performance, and microscope operator can introduce unwanted variability into the data~\cite{caicedo_data-analysis_2017,helen_pearson_image_2005,helen_pearson_good_2007}.
\end{enumerate}

\section{Potential Benefits and Impact}\label{sec:potential-benefits-and-impact}
Diffusion models offer promising potential for generating synthetic \gls{gl:brightfield-microscopy} images, which could significantly benefit cell detection tasks.
These models can produce diverse and realistic synthetic data, potentially alleviating the scarcity of labeled
training data that often slows down deep learning approaches in microscopy~\cite{dennis_eschweiler_denoising_2023,cross-zamirski_class-guided_2023}.
By learning to model the underlying distribution of cellular appearances, diffusion models may enhance the ability to
detect cells across varying imaging conditions and morphologies~\cite{alon_saguy_this_2023}.
This improved robustness is particularly valuable for \gls{gl:brightfield-microscopy}, where cell contrast and appearance can vary immensely.
Furthermore, diffusion models have demonstrated success in generating high-quality,
fully-annotated microscopy datasets without the need for manual annotations~\cite{cross-zamirski_class-guided_2023,dennis_eschweiler_denoising_2023}.
This capability could dramatically reduce the time and resources required for data preparation while still
enabling the training of accurate cell detection models.
Additionally, the synthetic data generated by diffusion models has shown potential for improving the performance of
downstream tasks, such as image-based profiling and classification~\cite{cross-zamirski_class-guided_2023}.
By leveraging these models to augment existing datasets, researchers may be able to develop more robust and
generalizable cell detection algorithms that perform well across a broader range of brightfield imaging conditions and cell types.

Developing more effective methods for single-cell detection in brightfield images could have far-reaching implications
for biological research and applications.
Improved brightfield cell detection would enable large-scale, \gls{gl:label-free} analysis, making high-throughput
screening and longitudinal studies of live cells easier, without the need for potentially cytotoxic fluorescent labels~\cite{srijit_seal_decade_2024,gregory_p_way_evolution_2023}.
More accurate cell detection could reveal subtle variations in morphology and behavior that are
masked in population-level analyses, advancing the understanding of cellular heterogeneity~\cite{gregory_p_way_evolution_2023}.
Additionally, enhanced capabilities for analyzing \gls{gl:label-free} cellular responses could streamline the drug
discovery process by enabling more physiologically relevant assays and accelerating compound screening~\cite{terra_m_kuhn_accessible_2024,jan_oscar_cross-zamirski_label-free_nodate}.
These advancements would collectively contribute to a more comprehensive and efficient approach to cellular
analysis across various fields of biomedical research.

\section{Ethical Considerations and Responsible Development}\label{sec:ethical-considerations-and-responsible-development}
While diffusion models offer promising capabilities for generating synthetic microscopy data,
their usage raises several important ethical considerations that must be carefully examined.
A primary ethical concern is ensuring proper consent and privacy protections when using real microscopy data to train diffusion models.
Even if the synthetic images generated are not exact copies of real data, the model may encode private or sensitive information from the training set~\cite{georgios_kaissis_secure_2020}.
Researchers must obtain appropriate consent and permissions to use real microscopy data for model training.
Additionally, synthetic data generation should be done in compliance with relevant data protection regulations like GDPR~\cite{david_restrepo_amariles_compliance_2020}.
Diffusion models can potentially amplify biases present in training data or introduce new biases~\cite{malsha_v_perera_analyzing_2023}.
For microscopy applications, this could manifest as the model generating images that are not representative of the full diversity of cell types,
tissue samples, or experimental conditions.
Biased synthetic data could then lead to biased or unfair performance of downstream object detection models~\cite{shuang_hao_synthetic_2024}.
Using synthetic microscopy images in scientific research and model development raises questions of scientific integrity and reproducibility.
There is a risk that artifacts or unrealistic features in synthetic images could lead to spurious or non-reproducible results in downstream analysis~\cite{maria_littmann_validity_2020}.
The ability to generate highly realistic synthetic microscopy images creates potential for misuse, such as fabricating
scientific results or evading detection of manipulated images~\cite{mirsky_creation_2020}.
While diffusion models are not uniquely susceptible to misuse, their powerful generative capabilities warrant
consideration of safeguards and responsible development practices.
When synthetic images are used in research or clinical applications,
there may be an ethical obligation to disclose their synthetic nature to human experts reviewing the images or to
patients whose diagnoses rely on \gls{ai} systems trained using synthetic data~\cite{danton_char_implementing_2018}.
Clear policies should be developed regarding disclosure and consent.
And lastly, training large diffusion models can have a significant computational cost and associated environmental impact~\cite{emma_strubell_energy_2019}.
Researchers should consider the energy usage and carbon footprint of synthetic data generation, and explore more efficient
architectures or transfer learning approaches where possible.

##################################################################################
################### chapters/05_methodology_unconditional_diffusion.tex ###################
##################################################################################

\chapter{Methodology: Image Generation}\label{ch:methodology-image-generation}
This chapter dives into the process of \gls{gl:brightfield-microscopy} image generation using unconditional diffusion models.
It outlines the comprehensive methodology, beginning with dataset acquisition and progressing through model architecture design,
training procedures, and evaluation techniques.
The chapter explores both objective metrics and subjective measures to assess the quality and realness of the generated images.
Ultimately, it leads up to the selection of the final model and optimized generation process that will be used to generate synthetic data for the proceeding cell detection step.


\section{Dataset Acquisition}\label{sec:diff-dataset-acquisition}
High-quality image acquisition is crucial for training effective diffusion models in image generation tasks.
The quality of the training data directly impacts the model's ability to learn and generate realistic images.
As noted by Ho et al.~\cite{ho_denoising_2020}, the forward process of adding noise to images relies on high-fidelity starting points to effectively learn the reverse denoising process.
Furthermore, Rombach et al.~\cite{rombach_high-resolution_2022} emphasize that the latent space representation in latent diffusion models benefits from clean,
high-resolution input images to capture fine details and textures.
The importance of image quality extends beyond just resolution; factors such as lighting, background, and camera settings
play a significant role in creating optimal training data.
Proper image acquisition techniques can help highlight desired features and minimize unwanted artifacts,
leading to more robust and accurate diffusion models.
As diffusion models continue to advance in generating high-quality synthetic images, ensuring the quality of the training
data becomes increasingly important to push the boundaries of what these models can achieve.

\subsection{Biological Setup}\label{subsec:biological-setup}
\acrfull{cho} cells, specifically the \gls{cho}-K1 and \gls{cho} DG44 variants, were used as the starting point for the experiments in this thesis.
These cells are widely used in biotechnology for producing proteins and other biological products~\cite{kim_cho_2012}, and thus serve as a great research base.
The cells were ``\gls{gl:stably-transfected}'' with a \gls{gl:plasmid}, which is a small circular piece of DNA~\cite{yusuf_b_johari_engineering_2022}
that has been permanently integrated into the cells' genetics.
This \gls{gl:plasmid} contained several important elements:
\begin{enumerate}
    \item A \gls{cmv} promoter: This is a genetic element that ensures strong and consistent expression of the \gls{egfp} gene~\cite{susanne_back_neuronal_2019}.
    \item A gene coding for \acrfull{egfp}: This protein glows green when exposed to blue or ultraviolet light,
    making it easy to visualize cells that have successfully incorporated the \gls{gl:plasmid}~\cite{roger_y_tsien_green_1998,riccardo_a_g_cinelli_enhanced_nodate}.
    \item Two \glspl{gl:antibiotic}: These genes allow cells that have taken up the \gls{gl:plasmid} to survive in the presence of specific antibiotics.
    One marker works in bacteria (used during the \gls{gl:plasmid} preparation process) and the other in mammalian cells like \gls{cho}~\cite{roger_y_tsien_green_1998,riccardo_a_g_cinelli_enhanced_nodate}.
\end{enumerate}
After introducing the \gls{gl:plasmid}, the cells were grown in a special growth medium containing antibiotics.
This step ensures that only cells that have successfully incorporated the \gls{gl:plasmid} survive, as they can resist the antibiotic due to the selection marker.
The cells were maintained in this selective environment until over 95\% of the population showed strong green fluorescence,
indicating successful and stable integration of the \gls{egfp} gene.

To prepare for imaging, the cells were counted to determine their concentration.
This step is crucial for calculating how much cell suspension to use when seeding the plates for \gls{ai} training.
Cell suspension refers to a liquid mixture where the cells are evenly dispersed in a solution.
This means that cells are floating freely in a liquid rather than being attached to a surface or clumped together.
The cells were then placed into 96-well plates (a standard format in biological research) at three different densities: 300, 150, and 1 cell per well.
This range of densities allows for the capture of images with varying cell numbers and distributions to create a diverse dataset, allowing the model
to see varying degrees of cell density.

Finally, the plates were imaged using a high-throughput microscope called CELLAVISTA 3.1 RS HE by Synentec GmbH~\cite{cellavista}.
This advanced instrument captured two types of images for each well: \textit{Brightfield} and \textit{Fluorescence} Images.
Brightfield images show the overall structure and position of cells, similar to what can be seen through a standard microscope (Fig.~\ref{fig:bf-sample}).
Whereas fluorescence images specifically highlight the cells expressing GFP, appearing as bright spots against a dark background (Fig.~\ref{fig:fluo-sample}).
The images were taken using a $10\times$ magnification objective lens, resulting in 8-bit grayscale images with a resolution of 3056x3056 pixels.
The imager, with its $10\times$ lens, captures multiple smaller regions of interest of the well, resulting in 20 total images per well.
These images, also called ``subwell images'', are mechanically stitched together in a circular pattern, resulting in an image of size $8192\times8192$, due to cropping of overlapping areas (Fig.~\ref{fig:full-well-bf} and ~\ref{fig:full-well-overlay}).
This approach was chosen to capture as much detail as possible, and to increase the relative area these small cells occupy in high-quality images.
\begin{figure}
    \begin{center}
        \begin{subfigure}{.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/bf_sample}
            \caption{Brightfield subwell image sample.}
            \label{fig:bf-sample}
        \end{subfigure}
        \begin{subfigure}{.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/fluo_sample}
            \caption{Fluorescence subwell image sample.}
            \label{fig:fluo-sample}
        \end{subfigure}
        \begin{subfigure}{.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/fullwell_bf}
            \caption{Stitched full well brightfield image, with no additional visualization effects.\newline\newline}
            \label{fig:full-well-bf}
        \end{subfigure}
        \begin{subfigure}{.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{images/fullwell_overlay}
            \caption{Stitched full well brightfield image with the fluorescence channel tinted and overlaid on top.
            Both have been normalized across all sub well images.}
            \label{fig:full-well-overlay}
        \end{subfigure}
    \end{center}
    \caption{Resulting images from brightfield and fluorescence imaging using the CELLAVISTA 3.1 RS HE.
    Typically, the fluorescence image is overlaid on top of the brightfield image and tinted to reflect the
    emission color of the \glslink{egfp}{eGFP} protein.}
    \label{fig:sample-cellavista-image}
\end{figure}

While \gls{gl:fluorescence-microscopy} images may not seem immediately relevant to this thesis focused on \gls{gl:brightfield-microscopy},
they play a crucial role in the data acquisition phase of the object detection model.
Although these fluorescence images are not utilized in the image generation process itself, they significantly facilitate the labeling of
cells for object detection purposes.
The fluorescent markers provide a clear contrast, making it easier to identify and localize individual cells and their boundaries accurately.
This enhanced visibility allows for more precise and efficient labeling, and ensures that the increased workload in the \gls{gl:cell-line} development step is worthwhile.
A more detailed explanation of this labeling process and its importance will be provided in subchapter~\ref{sec:det-dataset-acquisition}.

\subsection{Pre-Processing}\label{subsec:pre-processing}
The Pre-processing stage plays a crucial role in preparing the raw microscopy images for training diffusion models.
This phase involves several steps to optimize the dataset, ensuring that the images are suitable for the machine learning
task at hand while preserving the essential cellular information.

Following the imaging process described in the previous section, a total of roughly 900 subwell images were acquired, each with dimensions of $3056\times3056$ pixels.
To further increase the relative area occupied by cells in each image and to prevent data loss that might occur from resizing larger images,
these subwell images were split into smaller patches of $512\times512$ pixels.
This process resulted in approximately 32,000 patches, significantly increasing the dataset size while maintaining a high image quality.
It's worth noting that during the patch creation process, the last column and row of patches required black padding of 16 pixels on the right and bottom edge,
to ensure a consistent patch size across the dataset.
This minor adjustment ensures uniformity in the input data for the diffusion models.

One of the challenges encountered in the raw images was the presence of optical imaging artifacts caused by the shape of the wells in the \gls{gl:microtiter-plate}.
These artifacts manifested as dark arches in many subwell images (Fig.~\ref{fig:well-edge}), creating a thick border between the inside of the well and the plate itself.
They are caused by the refraction and reflection of light on the curved surfaces of the wells, which bend and scatter light in various directions.
Additionally, the curved geometry of the wells acts like lenses, focusing and defocusing light, leading to uneven illumination and shadowing effects.
This artifact could potentially interfere with the model's ability to detect and analyze cells accurately.
The appearance of these well edges varies considerably depending on the exact shape of the well (Fig.~\ref{fig:well-schematic}),
impacting the visibility of cells near the edge.
To address this issue and maintain the focus on cellular structures, a decision was made to exclude patches containing visible well edges from the dataset.
This filtering process reduced the total number of usable patches to approximately 24,000.
While this reduction in data quantity might seem substantial, it ensures that the remaining images contain primarily relevant cellular information,
which is crucial for training accurate diffusion models.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{.66\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/well_schematic}
        \caption{Schematic of the most common \gls{gl:microtiter-plate} well shapes.\newline\noindent(Image source: Auld et al.~\cite{auld2020microplate})}
        \label{fig:well-schematic}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/well_edge}
        \caption{Sample subwell image with the visible well edge.}
        \label{fig:well-edge}
    \end{subfigure}
    \caption{Illustration of various \gls{gl:microtiter-plate} well shapes and a subwell image demonstrating how the well edge can obscure the visibility of cells.}
    \label{fig:well-edge-problems}
\end{figure}
Given the computational constraints and the time limitations of this thesis, a further subset of 10,000 images was randomly selected from the 24,000 filtered patches.
This final dataset strikes a balance between having sufficient data for training different diffusion models and managing the computational resources available within the thesis timeframe.
To optimize the training process even further and reduce potential bottlenecks in data transfer, the selected images were converted into a Hugging Face dataset format~\cite{lhoest-etal-2021-datasets}.
This involved transforming the image data into parquet files, a columnar storage format that offers improved efficiency and maintainability during the training phase~\cite{vohra_apache_2016}.
This step is particularly important when dealing with large datasets, as it can significantly reduce the time spent on data loading and preprocessing during model training.

By implementing these pre-processing steps, a refined and optimized dataset has been created that is well-suited for training diffusion models.


\section{Model Architectures}\label{sec:diff-model-architecture}
This section details the rationale behind testing different model configurations and presents the specific architectures employed in this study,
whereas training details and further hyperparameters are explained in more detail in chapter~\ref{sec:diff-model-training}.

The exploration of various diffusion model architectures is crucial for understanding the impact of different structural
elements on the generation of single-cell microscopy images.
Firstly, it allows for a comprehensive understanding of how architectural choices influence the model's ability to
capture and reproduce the intricate details of single cells in microscopy images.
Secondly, by testing models with and without attention mechanisms, it becomes possible to assess whether the additional
computational cost of attention layers actually translates to meaningful improvements in image quality.
Lastly, experimenting with larger and smaller U-Net architectures (see~\ref{subsec:model-architectures}) provides insights
into the trade-offs between model complexity, computational requirements, and generation quality.

The Hugging Face Diffusers~\cite{von-platen-etal-2022-diffusers} library was utilized for implementing and training four distinct diffusion models: \textit{scc\_small\_attn}, \textit{scc\_small}, \textit{scc\_large} and \textit{scc\_medium\_attn}.
This choice was motivated by the library's robust implementation of state-of-the-art diffusion techniques and its
flexibility in allowing customization of model architectures and interchangeable noise schedulers to evaluate different diffusion speeds and output quality.

The \textit{scc\_small\_attn} model serves as the baseline architecture, incorporating a balance of standard convolutional blocks and attention blocks.
With 71.4 million trainable parameters out of a total of 95.3 million, this model required a substantial training duration of 7 days, 12 hours, and 40 minutes to reach the maximum training amount of 350 epochs (see chapter~\ref{sec:diff-model-training}).
The architecture includes attention blocks in both the down-sampling and up-sampling paths, potentially allowing for more complex feature interactions.

In contrast, \textit{scc\_small} eliminates all attention blocks, relying solely on standard convolutional operations.
This simplification barely reduces the number of trainable parameters to 70.1 million, but significantly shortens the training time to about a third compared to \textit{scc\_small\_attn}.
The comparison between \textit{scc\_small\_attn} and \textit{scc\_small} aims to elucidate the specific contributions of attention mechanisms in the context of single cell image generation.

The \textit{scc\_large} model represents a substantial increase in network capacity, featuring additional layers and a
higher number of channels in the later stages of the network.
With 277 million trainable parameters, this model is about four times larger than \textit{scc\_small} and explores the potential benefits of increased model complexity.
Although having a bigger architecture with much more trainable parameters, this model had a similar training time as \textit{scc\_small}.
This confirms that the self-attention blocks require much more computational resources than normal \gls{cnn} blocks.

Finally, \textit{scc\_medium\_attn} presents an intermediate approach, incorporating more attention blocks than \textit{scc\_small\_attn}
but maintaining a more moderate increase in overall network size compared to \textit{scc\_large}.
Due to time constraints and no further visual improvements in performance, the training process of this model was cancelled after 310 epochs.
With 114 million trainable parameters, and more self-attention blocks than \textit{scc\_small\_attn}, training this model for 40 more
epochs would have resulted in far more than 8 days of pure training.

The systematic variation in these architectures allows for an analysis of how different structural elements
contribute to the model's performance in generating realistic single cell microscopy images.
By comparing the outputs and performance metrics of these models, it becomes possible to draw informed conclusions
about the optimal architectural choices for this specific application,
potentially leading to more accurate and efficient single cell image generation systems.
\begin{table}[h!]
    \centering
    \begin{threeparttable}
        \caption{This table presents a detailed comparison of the diffusion model archictectures.}
        \begin{tabular}{p{2.1cm} | P{1.6cm} | P{1cm} P{1cm} | P{1.2cm} P{1.2cm} | P{1.2cm} | P{2cm}}
            \toprule
            Model Name                 & Block Out Channels                                                           & Down Block Types\tnote{1} & Up Block Types\tnote{2} & Total Params & Trainable Params & \multicolumn{1}{p{1.2cm}|}{\centering Epochs} & Training Duration \\
            \midrule
            \textit{scc\_small\_attn}  & 128\newline 128\newline 256\newline 256\newline 512                          & DB DB ADB DB DB           & UB UB AUB UB UB         & 95.3M        & 71.4M            & 350    & 7d 12h 40m  \\
            \midrule
            \textit{scc\_small}        & 128\newline 128\newline 256\newline 256\newline 512                          & DB DB DB DB DB            & UB UB UB UB UB          & 94M          & 70.1M            & 350    & 2d 7h 10m  \\
            \midrule
            \textit{scc\_large}        & 128\newline 128\newline 256\newline 256\newline 512\newline 512\newline 1024 & DB DB DB DB DB DB DB      & UB UB UB UB UB UB UB    & 300M         & 277M             & 350    & 2d 11h 30m \\
            \midrule
            \textit{scc\_medium\_attn} & 128\newline 128\newline 256\newline 256\newline 512\newline 512              & DB DB ADB DB ADB DB       & UB AUB UB AUB UB UB     & 128M         & 114M             & 310    & 7d 4h 50m \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \item [1] DB = DownBlock, ADB = AttnDownBlock
            \item [2] UB = UpBlock, AUB = AttnUpBlock
        \end{tablenotes}
    \end{threeparttable}\label{tab:table}
\end{table}

\section{Model Training}\label{sec:diff-model-training}
The training process for the diffusion models was designed to optimize performance while balancing computational constraints.
This section details the training configuration and hyperparameters used across all model architectures.

The training dataset comprised $10,000$ images, selected from a larger pool of preprocessed patches as described in section~\ref{subsec:pre-processing}.
This dataset size was chosen to provide a robust representation of the cell imagery while remaining computationally tractable within the
time constraints of this thesis.
Each model was trained for a maximum of 350 epochs, a duration chosen through testing to allow for convergence while reducing overfitting risks.
The learning rate was set at $0.0001$, a value commonly used in diffusion model training that provides a balance between convergence speed and stability.
The AdamW optimizer was selected for its ability to handle sparse gradients effectively, which is particularly beneficial in the context of diffusion models.
The \acrfull{ddim} scheduler was employed for its efficiency and improved sampling quality compared to traditional diffusion processes.
An \acrfull{ema} decay of $0.9999$ was applied to the model weights, a technique known to stabilize training and potentially improve generalization.
A batch size of 4 was utilized, optimized for the available \gls{gpu} memory while trying to ensure sufficient stochasticity in gradient updates.

To facilitate model evaluation and enable the selection of optimal checkpoints, the training process incorporated regular checkpointing and validation.
Checkpoints were saved every 10 epochs, allowing for analysis of model performance over time.
Concurrently, \acrfull{fid} validation was performed every 10 epochs, providing quantitative metrics
on the quality and diversity of generated images throughout the training process.
For this purpose, a subset of 128 sample images was generated.
These images were produced using 25 inference steps and the \gls{ddim} scheduler, providing a consistent basis for evaluating model progress and performance.
Torchmetrics~\cite{nicki_skafte_detlefsen_torchmetrics_2022} was used to calculate the \gls{fid} value.

The training infrastructure leveraged \acrfull{ddp} across two NVIDIA A100-SXM4-40GB \gls{gpu},
significantly accelerating the training process.
Mixed precision training with $float16$ was implemented to further enhance computational efficiency without compromising model quality.
PyTorch Lightning~\cite{falcon_pytorchlightningpytorch-lightning_2020} was used to reduce boilerplate code and facilitate easy scaling to multi-\gls{gpu} training scenarios.
This framework also seamlessly integrated mixed precision training and other optimization techniques.

This comprehensive training configuration was applied consistently across all model architectures described in section~\ref{sec:diff-model-architecture}.
By maintaining consistency in the training process, meaningful comparisons could be drawn between the performance of
different model architectures, isolating the impact of architectural choices on the quality of generated cell images.

\section{Model Evaluation}\label{sec:diff-model-evaluation}
The evaluation of the trained diffusion models is a critical step in assessing their performance and suitability for generating high-quality images.
In the context of this thesis, where the final model will be used to generate samples for object detection,
a comprehensive evaluation approach combining both objective and subjective measures is essential.
The use of both objective and subjective metrics is necessary for several reasons.
Objective metrics provide quantifiable and reproducible results, allowing for consistent comparisons between different models and across various experiments.
They offer a standardized way to assess model performance and track improvements over time.
However, these metrics may not always capture the nuanced aspects of image quality that are important for human perception~\cite{george_juraj_stein_exposing_2023,sadeep_jayasumana_rethinking_2023}.
Subjective metrics, on the other hand, directly incorporate human judgment and can capture subtle qualities that objective measures might miss.
They are particularly valuable in assessing the realism and perceptual quality of generated images,
which is crucial for applications in microscopy and biological imaging.
However, subjective evaluations can be time-consuming, expensive, and potentially biased~\cite{pedram_mohammadi_subjective_2014}.
By combining both approaches, the strengths of each method can be leveraged to gain a more comprehensive understanding of the model's performance.

\subsection{Objective Metrics}\label{subsec:diff-objective-metrics}
The primary objective metric used in this evaluation is the \gls{fid}.
\gls{fid} has become a standard metric in the field of generative models due to its ability to
capture both the quality and diversity of generated images~\cite{george_juraj_stein_exposing_2023}.
\gls{fid} measures the distance between the feature distributions of real and generated images.
It uses the Inception-v3 network, pre-trained on ImageNet, to extract features from both sets of images.
The feature vectors are then modeled as multivariate Gaussian distributions,
and the Fréchet distance between these distributions is calculated~\cite{sadeep_jayasumana_rethinking_2023}:
\begin{equation}
    FID = \|\mu_r - \mu_g\|^2 + Tr(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
    \label{eq:fid}
\end{equation}
Where $\mu_r$ and $\mu_g$ are the mean feature vectors for real and generated images, respectively, and $\Sigma_r$ and $\Sigma_g$ are their covariance matrices.
$Tr$ denotes the trace of a matrix as a scalar value, meaning the sum of the elements on its main diagonal.
It is used to measure the difference in the covariances of the real and generated data distributions.
\gls{fid} is preferred over other metrics for several reasons~\cite{eyal_betzalel_evaluation_2024,ivana_marin_evaluation_2020}:
\begin{enumerate}
    \item It correlates well with human perception of image quality.
    \item It has become one of the most commonly used metric in the field.
    \item It is sensitive to both the quality and diversity of generated images.
\end{enumerate}
However, it's important to note that \gls{fid} has some limitations.
Recent research has shown that it may not always reflect the state-of-the-art perceptual realism of diffusion models as judged by humans~\cite{george_juraj_stein_exposing_2023}.
Additionally, its reliance on the Inception-v3 network, which was trained on natural images, may not be ideal for specialized domains like microscopy images~\cite{sadeep_jayasumana_rethinking_2023}.

In addition to the Fréchet Inception Distance, several other objective metrics have been proposed for evaluating generative models.
However, \gls{fid} remains the most widely used due to its balance of effectiveness and practicality.
Some alternative metrics and brief explanations of why they may be less suitable than \gls{fid} for this thesis are described below:
\textbf{\acrfull{is}}: This metric also uses the Inception-v3 network to measure both the quality and diversity of generated images.
While it was once popular, \gls{is} has several limitations~\cite{barratt_shane_note_2018,Ravuri2019SeeingIN}:
\begin{itemize}
    \item It doesn't compare generated images to real ones, making it less suitable for assessing realism.
    \item It can be easily fooled by mode collapse, where a model generates a small set of high-quality but limited-diversity images.
    \item It's less reliable for domains significantly different from ImageNet, such as microscopy images.
\end{itemize}
\textbf{\acrfull{kid}}: Similar to \gls{fid}, \gls{kid} measures the distance between real and generated image features.
However, it's computationally more expensive than \gls{fid} and it may not provide significant advantages over \gls{fid} for the specific microscopy image domain~\cite{zixiao_wang_distributed_2023,sadeep_jayasumana_rethinking_2023}. \\
\textbf{Wasserstein Distance}: This metric measures the cost of transforming one distribution into another.
It can also be computationally intensive, especially for high-dimensional data like images, while also being less intuitive to interpret~\cite{meysam_cheramin_computationally_2022,alex_lipp_short_2023}. \\
\textbf{\acrfull{msssim}}: This metric focuses on structural similarities between images.
But it focuses too much on luminance and structural information, potentially overlooking color-related aspects and does not align well with human judgement of image quality~\cite{fitri_n_rahayu_ss-ssim_2009,rafal_mantiuk_hdr-vdp-2_2011}.

\subsection{Subjective Measures}\label{subsec:diff-subjective-measures}
To complement the objective evaluation, a subjective assessment was conducted using a survey created with SurveyJS~\cite{noauthor_surveyjssurvey-library_2024} (Fig.~\ref{fig:survey1} and~\ref{fig:survey2}).
The survey (Appendix~\ref{ch:survey}) included a set of sample images, consisting of 20 generated images (Tab.~\ref{tab:gen_images}) and 10 real images (Tab.~\ref{tab:real_images}) in random order.
The images were generated by the final model and hyperparameter set concluded in section~\ref{sec:final-model-decision}.
This survey was distributed to microscopy imaging experts and biologists from Synentec GmbH.
The participants were asked to:
\begin{enumerate}
    \item Decide whether each image was real or generated.
    \item State the confidence in their decision.
    \item If they decided an image was fake, explain what led to their decision.
\end{enumerate}
This approach allows for a nuanced evaluation of the generated images, capturing aspects that might not be reflected in objective metrics.
It provides insights into the model's ability to generate images that are indistinguishable from real microscopy images,
which is crucial for its intended application in object detection.

\section{Final Model Decision}\label{sec:final-model-decision}
The evaluation of trained diffusion models with different noise schedulers and hyperparameters is a crucial step in
optimizing the performance of generative models.
This process is particularly important because the chosen combination will significantly impact the quality
and characteristics of the generated images, which will subsequently be used for object detection tasks.
Testing various hyperparameters and noise schedulers during inference is essential for several reasons.
Firstly, different schedulers can affect the trade-off between image quality and generation speed~\cite{jiajie_fan_noise_2023}.
Secondly, they can influence the plausibility and semantic expression of the generated images~\cite{jiajie_fan_noise_2023}.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{images/fid_chart}
    \caption{\glslink{fid}{FID} metric comparison of the different diffusion architectures throughout the training process.
    The top 5 lowest \glslink{fid}{FID} checkpoints of each model are marked with a star.}
    \label{fig:fid-chart}
\end{figure}
In this study, the top 5 lowest \gls{fid} checkpoints (see Fig.~\ref{fig:fid-chart}) of every model architecture (chapter~\ref{sec:diff-model-architecture})
were tested with multiple state-of-the-art noise schedulers and different hyperparameters.
The schedulers evaluated included DPM++ 2M, DPM++ 2M Karras, DPM++ 2M SDE, DPM++ 2M SDE Karras, DPM++ SDE, DPM++ SDE Karras, Euler, Euler a, and \gls{ddim}.
Each scheduler was tested with varying inference steps, ranging from 10 to 45 in increments of 5 for all schedulers except \gls{ddim},
which was tested from 30 to 65 in increments of 5.
Additional hyperparameters that were explored include:
\begin{itemize}
    \item \textbf{Solver order}: This parameter, set to either 2 or 3, determines the order of the numerical solver used in the diffusion process.
    It is recommended to use $solver\_order=2$ for guided sampling, and $solver\_order=3$ for unconditional sampling.
    \item \textbf{Prediction type}: Two options were tested: $epsilon$ and $v\_prediction$.
    The $epsilon$ prediction focuses on predicting the noise added to the image,
    while \linebreak
    $v\_prediction$ is an alternative formulation that offers more numerical stability and might avoid color-shifting artifacts in higher resolution samples.
    \item \textbf{Timestep spacing}: This parameter, set to either $linspace$ or $trailing$, determines how the timesteps are distributed during the diffusion process.
    $Linspace$ spacing distributes timesteps evenly, while $trailing$ spacing concentrates more steps towards the end of the process.
\end{itemize}
Each combination of these hyperparameters was used to generate 8 samples, providing a comprehensive set of images for evaluation.
The final model, scheduler, and hyperparameter combination was handpicked based on a careful examination of all generated samples,
with the goal of selecting the configuration that produced the best-looking images.

This approach to hyperparameter optimization aligns with recent research in the field.
For instance, studies have shown that the choice of noise schedule can significantly impact the plausibility
of generated designs, with some schedules improving the rate of plausible designs from 83.4\% to 93.5\%~\cite{jiajie_fan_noise_2023}.
By meticulously evaluating these various configurations, it can be ensured that the chosen model and parameters are optimized for
generating high-quality, diverse, and semantically coherent images.
This optimization is crucial for the subsequent object detection tasks, as the quality and characteristics of the
generated images will directly impact the performance and reliability of the detection algorithms.

After thorough evaluation of the various model architectures, noise schedulers, and hyperparameters,
a final configuration was selected for the cell detection task.
The chosen model is the \textit{scc\_small} architecture, which demonstrated a good balance between performance and computational efficiency.
This model, despite its relatively smaller size (70.1M trainable parameters), achieved competitive results in terms of image quality and fidelity.

The selected noise scheduler for the final configuration is Euler a, which is known for its ability to produce high-quality
samples with a good trade-off between speed and image fidelity.
The choice of trailing timestep spacing allows for a more concentrated sampling towards the end of the diffusion process,
potentially leading to improved fine details in the generated images.
The prediction type was set to epsilon, focusing on predicting the noise added to the image during the diffusion process.

For the generation of the final dataset to be used in the cell detection task, a dynamic approach was implemented.
The inference steps were randomly chosen between 35 and 40 for each batch of 16 images.
A total of 10,000 images were generated.
This variation in inference steps introduces a subtle level of diversity in the generation process,
potentially benefiting the robustness of the subsequent detection model.

The final calculated \gls{fid} score for this configuration was 45.57, which represents a significant improvement over the
initial \gls{fid} scores observed during training (Fig.~\ref{fig:fid-chart}).
This improvement might be be attributable to the careful selection of the noise scheduler and other hyperparameters,
as well as the increased number of inference steps compared to the validation setup.
It's worth noting that the final decision was solely influenced by subjective evaluation of the samples.
The chosen configuration produced images that were deemed to have the best visual quality and most closely resembled the
characteristics of real microscopy images of single cells.
This optimized model and generation pipeline was then used as the foundation for creating a diverse and high-quality dataset of synthetic cell images.
These images were crucial in the next phase of the thesis, where they were used to train and evaluate object
detection models for single-cell detection in microscopy images.

###########################################################################
################### chapters/06_methodology_object_detection.tex ###################
###########################################################################

\chapter{Methodology: Cell Detection}\label{ch:methodology-cell-detection}
In the field of object detection, datasets typically consist of images paired with corresponding text files containing
bounding box information for the objects of interest.
Over time, several standardized formats have emerged for storing this label data, each with its own strengths and use cases.

Formerly one of the most widely used formats is the \gls{pascal} \gls{voc} format, which has been influential in
the development of many object detection models, including \gls{ssd}, \gls{rcnn}, Fast \gls{rcnn}, and Faster \gls{rcnn} (chapter~\ref{subsec:history-of-object-detectors}).
The \gls{pascal} \gls{voc} format stores annotation data in XML files, providing a structured and human-readable representation of object locations.
Each XML file contains detailed information about the image, including filename, size, and object annotations.
The bounding box coordinates are represented as $[x\_min, y\_min, x\_max, y\_max]$, defining the top-left and bottom-right corners of the box.

Another popular format is the \gls{yolo} format, which is closely associated with the \gls{yolo} family of object detection models.
\gls{yolo} annotations are stored in simple text files, with each line representing a single bounding box and each value in a line being separated by a whitespace.
The format is concise and easy to parse, using normalized coordinates: $[class\_id, x\_center, y\_center, width, height]$ .
This normalization allows for easier scaling and processing of annotations across different image sizes.

The \acrfull{coco} format, developed by Microsoft, offers a more comprehensive approach to object annotation.
\gls{coco} annotations are stored in JSON files, providing a flexible and extensible structure for complex datasets, while also being human-readable.
The bounding box information is represented as $[x, y, width, height]$, where $(x, y)$ defines the top-left corner of the box.
\gls{coco}'s format allows for additional features such as segmentation masks and keypoints,
making it suitable for a wide range of computer vision tasks beyond object detection.

Each of these formats has its advantages and disadvantages.
The \gls{pascal} \gls{voc} format's XML structure provides rich metadata and is human-readable, but it can be more verbose and slower to parse for large datasets.
The \gls{yolo} format's simplicity makes it efficient for processing and storage, but it may lack some of the detailed metadata found in other formats.
The \gls{coco} format offers the most flexibility and feature-richness, but its complexity can be overwhelming for simpler projects.
The choice of annotation format often depends on the specific requirements of the project, the models being used, and the available tools and workflows.
For instance, researchers working with \gls{rcnn}-based models might prefer the \gls{pascal} \gls{voc} format for its compatibility with existing codebases.
Those focusing on real-time object detection might opt for the \gls{yolo} format due to its efficiency and direct integration with \gls{yolo} models.
Projects requiring advanced annotations or working with diverse datasets might benefit from the extensibility of the \gls{coco} format.

For this research the \gls{yolo} format was chosen, as it aligns best with the object detection models that were trained (chapter~\ref{sec:det-model-selection}).

\section{Dataset Acquisition}\label{sec:det-dataset-acquisition}
The creation of object detection datasets is significantly more time-consuming than assembling datasets for unconditional diffusion models,
primarily due to the need for detailed label data.
While the base dataset described in Section~\ref{subsec:pre-processing} provided the necessary images,
it lacked the crucial label data needed for training an object detection model.
To address this, a comprehensive labeling process was undertaken to prepare the data for the object detection task.

\begin{table}[h!]
    \centering
    \caption{Baseline and mixed datasets used for object detection model training.}
    \label{tab:datasets}
    \begin{tabular}{l|c|c|c|c}
        \toprule
        Dataset            & Real Images & Generated Images & Val Images & Test Images \\
        \midrule
        \textit{scc\_real} & 5000        & 0                & 2,527      & 16,758      \\
        \textit{scc\_10}   & 4500        & 500              & 2,527      & 16,758      \\
        \textit{scc\_30}   & 3500        & 1500             & 2,527      & 16,758      \\
        \textit{scc\_50}   & 2500        & 2500             & 2,527      & 16,758      \\
        \bottomrule
    \end{tabular}
\end{table}
Four distinct datasets were created (Tab.~\ref{tab:datasets}), each containing a total of 5,000 labeled images for training.
The remaining images were allocated to validation and test splits, maintaining consistency across all four datasets.
The composition of these datasets varied in terms of the ratio between real and generated images.
For all datasets, the validation split consisted of 2,527 images, while the test split comprised 16,758 images.

The decision to use a large number of test images in the dataset can be highly beneficial for several reasons:
\begin{itemize}
    \item Statistical Significance: A large test set provides more statistically significant results.
    With 16,758 images, the performance metrics derived from the test set are likely to be more robust and reliable,
    reducing the impact of random variations or outliers.
    \item Diverse Scenarios: In microscopy, cell appearances can vary greatly due to factors like cell cycle stage, environmental conditions, or imaging parameters.
    A large test set is more likely to encompass a wide range of scenarios, ensuring that the model's performance is evaluated across diverse conditions.
    \item Confidence in Generalization: With a substantial test set, there's greater confidence that the model's performance metrics reflect
    its true generalization capability rather than being overly influenced by the specific composition of a smaller test set.
    \item Confidence in Small Improvements: When comparing different models or techniques, a large test set allows for the
    detection of small but statistically significant improvements that might not be apparent with a smaller dataset.
\end{itemize}

\subsection{Real Images}\label{subsec:real-images}
As mentioned in Section~\ref{subsec:biological-setup}, the fluorescence images, which were not utilized during the diffusion training process,
played a crucial role in the labeling of real images.
The availability of these fluorescence images significantly sped up the labeling process for the \gls{gl:brightfield-microscopy} images.

To leverage the fluorescence data, a simple classical image processing algorithm was developed using OpenCV~\cite{opencv_library}.
This algorithm was designed to detect cells in the binarized fluorescence mask (as shown in Fig.~\ref{fig:fluo-sample}).
Bounding boxes were then extracted from the detected contours.
The process can be summarized by the simplified pseudocode of Algorithm~\ref{alg:cell-detection}.
\begin{algorithm}
    \caption{Cell Detection and Bounding Box Extraction Pseudocode}
    \label{alg:cell-detection}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Fluorescence image $F$
        \State \textbf{Output:} List of bounding boxes $B$
        \State $B \gets \emptyset$
        \State $F_{bin} \gets \text{Binarize}(F)$
        \State $contours \gets \text{FindContours}(F_{bin})$
        \For{each $contour$ in $contours$}
            \If{$\text{Area}(contour) > \text{min\_area}$}
                \State $box \gets \text{BoundingRect}(contour)$
                \State $B.\text{append}(box)$
            \EndIf
        \EndFor
        \State \textbf{return} $B$
    \end{algorithmic}
\end{algorithm}
This algorithm provides a foundation for automated cell detection, significantly reducing the manual labeling effort.
However, to ensure accuracy, the detected bounding boxes were manually checked and, when necessary, slightly adjusted.
This approach combines the efficiency of automated detection with the precision of human verification,
resulting in a high-quality labeled dataset of real images.

\subsection{Generated Images}\label{subsec:generated-images}
The acquisition of the generated images was discussed in more detail in chapter~\ref{sec:final-model-decision}, and only
missing are the bounding boxes for the cells in these images.
To facilitate the labeling process for these images, a \gls{yolo}v8m model was quickly trained on the previously labeled real image data.
This pre-trained model was then used in conjunction with Roboflow's ``Model-Assisted Labeling''~\cite{dwyer_roboflow_2024} feature, where the
selected model will run when an image is opened in the annotation tool, streamlining the labeling process.

While this approach significantly accelerated the labeling process compared to complete manual labeling, it was not without its limitations.
The model's predictions often required minor adjustments, particularly in cases where it detected dead-looking cells, misclassified generated debris as cells or simply missed them.
This process not only aided in efficient labeling but also served as an initial indicator of the quality and realism of the generated images.
The need for adjustments and the types of errors encountered provided first valuable insights into the strengths and weaknesses of the diffusion-based image generation process.
From the initial set of 10,000 generated images, 2,500 were carefully selected and labeled for inclusion in the mixed datasets.
This selection process ensured that only the highest quality generated images were incorporated into the training data,
maintaining the integrity of the dataset while exploring the impact of synthetic data on model performance.

\section{Model Architectures}\label{sec:det-model-selection}
When selecting an object detection model for single cell detection in \gls{gl:brightfield-microscopy} images, several factors must be taken into account:
\begin{enumerate}
    \item Accuracy: The model should be capable of detecting cells with high precision and recall, even in challenging scenarios such as crowded fields or low-contrast images.
    \item Speed: Real-time or near-real-time performance is often desirable in microscopy applications, especially for live-cell imaging or high-throughput screening.
    \item Scalability: The model should be able to handle varying image sizes and cell densities without significant performance degradation.
    \item Robustness: Given the potential variability in cell appearance and imaging conditions, the model should be resilient to noise, illumination changes, and other artifacts.
    \item Computational requirements: The hardware available for training and inference should be considered, as some models may require substantial computational resources.
    \item Ease of use and community support: Models with well-documented implementations and active community support can facilitate easier integration and troubleshooting, especially for researchers
    with limited experience in machine learning or computer vision.
\end{enumerate}
Based on these considerations and the specific requirements of this thesis,
three state-of-the-art models were selected for evaluation: \gls{yolo}v8, \gls{yolo}v9, and \gls{rtdetr}\@.
Each of these models offers unique advantages and has shown promising results in various object detection benchmarks.

\subsection{YOLOv8}\label{subsec:yolov8}
\gls{yolo}v8, developed and published by Ultralytics in january 2023, builds upon the success of \gls{yolo}v5 while introducing several key improvements and extensions~\cite{jocher_ultralytics_2023}.
The architecture of \gls{yolo}v8 is characterized by its efficient backbone, neck, and head components, which work in unison to achieve high accuracy and speed.
The backbone of \gls{yolo}v8 utilizes a CSPDarknet53 variant, which employs cross-stage partial connections to enhance feature reuse and gradient flow.
This design choice contributes to improved accuracy without significantly increasing computational complexity~\cite{terven_comprehensive_2023}.
\gls{yolo}v8 uses CIoU~\cite{zheng_distance-iou_2019} and DFL~\cite{li_generalized_2020} loss functions as the two main losses.
These losses help in detecting objects of varying sizes, especially smaller objects, which is particularly beneficial
for cell detection tasks where cell sizes may vary, but are always small compared to the image~\cite{terven_comprehensive_2023}.
The head of \gls{yolo}v8 employs an anchor-free detection mechanism (chapter~\ref{subsec:overview-od}), moving away from the anchor-based approach of earlier \gls{yolo} versions.

Additional improvements are available during the training process of \gls{yolo}v8.
Mosaic Augmentation is an advanced data augmentation technique that combines multiple images into a single training sample,
enhancing the model's ability to detect objects in various contexts~\cite{Bochkovskiy2020YOLOv4OS}.
\gls{yolo}v8's head is decoupled, enabling an independent processing of objectiveness, classification and regression.
This allows for more efficient training and inference, as the model can focus on specific tasks without interference from other components~\cite{terven_comprehensive_2023}.

For this thesis, \gls{yolo}v8 models of sizes s, m, and x were trained and evaluated.
These different sizes offer a range of trade-offs between accuracy and computational efficiency,
allowing for a comprehensive assessment of the model's performance in the context of single cell detection.

\subsection{YOLOv9}\label{subsec:yolov9}
The successor of \gls{yolo}v8, \gls{yolo}v9 was published by Wang et al.~\cite{Wang2024YOLOv9LW} in February 2024 and aims to
push the boundaries of object detection performance further.

Its architecture is characterized by several innovative features and improvements~\cite{Wang2024YOLOv9LW}:
\begin{itemize}
    \item Programmable Gradient Information (PGI): This novel concept allows the model to generate reliable gradients through
    auxiliary reversible branches, ensuring that deep features can correctly maintain the key characteristics for the target task.
    \item Enhanced Backbone: \gls{yolo}v9 employs a refined backbone structure that incorporates a more generalized version of
    the Efficient Layer Aggregation Network (GELAN).
    \item Information Bottleneck Principle: By leveraging PGI, essential data can be preserved across the network's depth,
    that otherwise might be lost during passes through successive layers of the network.
    \item Reversible Functions: To further prevent information degradation --particularly in its deeper layers-- \gls{yolo}v9 integrates
     reversible functions into its architecture, ensuring the preservation of vital data for object detection tasks.
\end{itemize}

For this thesis, \gls{yolo}v9 models of sizes c and e were trained and evaluated.
These models represent different points on the accuracy-efficiency spectrum, allowing for a comprehensive assessment
of \gls{yolo}v9's capabilities in single cell detection tasks.

\subsection{RT-DETR}\label{subsec:rt-detr}
\gls{rtdetr}, or Real-Time Detection Transformer, marks a significant shift from the \gls{cnn}-based architectures that have
dominated the \gls{yolo} family of object detection models.
Introduced by Zhao et al.~\cite{Lv2023DETRsBY} in 2023, RT-DETR aims to seamlessly blend the global reasoning
capabilities of transformers with the computational efficiency characteristic of \gls{cnn}-based detectors.
This innovative approach represents a promising direction in the field of object detection,
particularly for applications such as single cell detection in microscopy images.

The architecture of \gls{rtdetr} comprises several key components designed for efficient real-time object detection.
At its foundation lies a \gls{cnn}-based backbone, which extracts multi-scale features from the input image.
Specifically, the last three stages of the backbone (S3, S4, S5) are used as input to the subsequent stages~\cite{Lv2023DETRsBY}.
Following the backbone is an efficient hybrid encoder, which transforms the multi-scale features into a sequence of image features.
This hybrid encoder employs two main modules: The Attention-based Intra-scale Feature Interaction (AIFI) for processing the S5 features,
and the \gls{cnn}-based Cross-scale Feature-fusion Module (CCFM) for fusing features from S4 and S5~\cite{Lv2023DETRsBY}.
This feature fusion strategy strikes a balance between computational efficiency and the need to capture multi-scale information.
It is particularly relevant in the context of cell detection, where cells may appear at various scales within a single image due to factors such as depth of field or natural size variations.
The use of both attention mechanisms and convolutional operations in the encoder allows \gls{rtdetr} to capture both local and global contextual information efficiently.
Following the encoder, an \gls{iou}-aware query selection module chooses a set of initial object queries,
which are then refined by a Transformer decoder with auxiliary prediction heads to generate the final bounding boxes and confidence scores.

One of the standout features of \gls{rtdetr} is its end-to-end design, eliminating the need for non-maximum suppression (chapter~\ref{subsec:history-of-object-detectors}).
As mentioned in previous chapters, the removal of \gls{nms} in post-processing steps enables the model to generate high-quality predictions directly,
while maintaining real-time performance.

For the same reasons mentioned previously, \gls{rtdetr} models of sizes x and l were trained and evaluated.
\section{Model Training}\label{sec:det-model-training}
The training process for the object detection models in this study utilized the Ultralytics framework as the primary tool~\cite{jocher_ultralytics_2023}.
This choice was made due to its robust implementation of state-of-the-art models and its flexibility in handling various training configurations.
All models were fine-tuned on the datasets described in chapter~\ref{sec:det-dataset-acquisition}, rather than being trained from scratch.
The base models had been pre-trained on the \gls{coco} dataset, a large-scale object detection dataset comprising 80 classes, as discussed in chapter~\ref{subsec:datasets-and-performance-metrics}.

\subsection{Fine-Tuning Approach}\label{subsec:fine-tuning-approach}
The decision to fine-tune pre-trained models instead of training from scratch offers several advantages.
Transfer learning, the process of using knowledge gained from solving one problem and applying it to a different but related problem,
is at the core of fine-tuning~\cite{kurkova_survey_2018}.
In the context of object detection, fine-tuning allows the model to leverage general features learned from a large, diverse dataset,
like shapes, and adapt them to the specific task at hand.
One of the primary benefits of fine-tuning is the significant reduction in training time and computational resources required.
Pre-trained models have already learned a rich set of features from a large dataset, which can be particularly beneficial when working with smaller,
domain-specific datasets~\cite{yosinski_how_2014}.
This is especially relevant in the current study, where the dataset size is relatively small compared to large-scale datasets like \gls{coco}\@.
Additionally, fine-tuning often leads to better generalization and performance, particularly when the target dataset is limited in size or diversity.
The pre-trained weights serve as a good initialization point, allowing the model to converge faster and potentially to
a better optimum than if it were trained from random initialization~\cite{kornblith_better_2018}.
Another advantage of fine-tuning is its ability to mitigate overfitting.
When training complex models on smaller datasets, there's a risk of the model memorizing the training data rather than learning generalizable features~\cite{he_rethinking_2018}.

\subsection{Training Configuration}\label{subsec:training-configuration}
The training process was conducted using a single NVIDIA GeForce RTX 3090 \gls{gpu} with 24GB of memory.
This hardware configuration allowed for efficient training of the various model architectures and sizes.
Due to the restricted time frame of this thesis, an extensive hyperparameter search was not conducted.
Instead, the training configurations were based on the default settings provided by the Ultralytics framework\footnote{
\href{https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/default.yaml}{ultralytics/ultralytics/cfg/default.yaml at main · ultralytics/ultralytics · GitHub}},
with a batch size of 32, maximum epochs of 200 and early stopping patience of 35 epochs.
This batch size was chosen as a balance between memory constraints and training efficiency.
A larger batch size can lead to more stable gradient estimates and potentially faster convergence,
but it also requires more memory~\cite{smith_dont_2017}.
The maximum number of epochs was set to 200 to allow sufficient time for model convergence with the limited data available,
while the early stopping patience of 35 epochs was implemented to prevent overfitting and
unnecessary computation if the model's performance plateaued.

Ultralytics default data augmentation techniques were employed to artificially expand the training dataset
and improve the model's ability to generalize.
The following augmentation parameters were used:
\begin{itemize}
    \item HSV-Hue augmentation: 0.015
    \item HSV-Saturation augmentation: 0.7
    \item HSV-Value augmentation: 0.4
    \item Image translation: ±0.1 (fraction)
    \item Image scale: ±0.5 (gain)
    \item Left-right flip probability: 0.5
    \item Mosaic augmentation probability: 1.0
\end{itemize}
These augmentation techniques were chosen to introduce variability in the training data without distorting the essential characteristics of the cells.
The Hue and Saturation augmentations should have no effect on truly grayscale images.
The Value augmentation can slightly change the brightness of the images, simulating variations in illumination.
Translation and scaling augmentations simulate variations in cell position and size, which are common in microscopy images.
The left-right flip augmentation introduces reflection invariance, while the mosaic augmentation combines multiple images,
potentially helping the model learn to detect cells in various contexts and densities.

Notably, shear, and perspective transformations were not applied (all set to 0.0) to preserve the natural shape of the cells,
which are important features in \gls{gl:brightfield-microscopy} images.
Similarly, channel swapping (BGR) was not used, as this transformation might introduce unrealistic
variations that do not reflect the expected appearance of cells in the microscopy setup.

\section{Model Evaluation}\label{sec:det-model-evaluation}
Models trained on datasets partially composed of generated images (90\%, 70\%, and 50\% real images) were systematically compared against the baseline (100\% real images).
This comparison aims to effectively measure any improvements or degradations in model performance resulting from the inclusion of synthetic data.

The primary method for evaluating object detection model performance is through the comparison of standardized metrics (chapter~\ref{subsec:datasets-and-performance-metrics}).
As already mentioned, while various metrics exist, \gls{map} stands out as the most commonly and widely accepted metric in the field of object detection~\cite{tsung-yi_lin_microsoft_2014}.
Although \gls{map}@50:95 is often used to determine overall model precision in localization and accuracy, this study places particular emphasis on mAP@50.
The rationale behind this focus lies in the specific requirements of single cell detection in \gls{gl:brightfield-microscopy} images.

In the context of this research, the primary objective is to accurately detect the presence and approximate location of individual cells.
Unlike tasks that require precise morphological analysis of the detected cells afterwards, the exact tightness of the bounding box is of lesser importance.
As long as the cell is correctly identified and roughly localized, the detection can be considered successful for the purpose of this study.
The chosen metric (\gls{map}@50) is well-suited for this scenario, as it evaluates the model's performance based on a more lenient Intersection over Union (IoU) threshold of 0.5.
In addition, this model is less sensitive to cell size variations compared to higher \gls{iou} thresholds.
It is important to note that while \gls{map}@50 is the primary focus, the other \gls{map} variants are not disregarded.
These metrics provide valuable supplementary information about the models' performance, particularly in cases where more precise localization
may be beneficial for downstream analysis or future applications.

While \gls{map} serves as the primary quantitative metric, the evaluation process also considers several qualitative and practical aspects:
\begin{enumerate}
    \item Inference speed: Assessment of the model's processing time per image, which is crucial for potential real-time applications in microscopy workflows and high-throughput screening.
    \item Model size and computational requirements: Evaluation of the trained models' memory footprint and computational demands, considering potential deployment constraints in research or clinical settings.
    \item Sample detections: Curated sets of images showcasing successful detections, failure cases, and comparisons between models, providing qualitative insights into model behavior.
\end{enumerate}

The comprehensive evaluation methodology described in this chapter aims to provide a thorough and nuanced understanding
of the impact of incorporating generated images into the training data for single cell detection models.
By combining quantitative metrics, and qualitative assessments, this approach enables a robust evaluation
of the potential benefits and limitations of leveraging unconditional diffusion-based image generation in the context
of \gls{gl:brightfield-microscopy} cell detection.
The results obtained through this evaluation process will directly address the research question (chapter~\ref{sec:research-question}),
shedding light on whether and to what extent generated images can enhance object detection accuracy or reduce the reliance on large datasets of real images.
These findings have significant implications for the fields of biological image analysis and machine learning,
potentially offering new strategies for developing robust cell detection models with limited real-world data.

######################################################
################### chapters/07_results.tex ###################
######################################################
\chapter{Results}\label{ch:results}
This chapter presents the results of the conducted survey (Chapter~\ref{subsec:diff-subjective-measures}),
visual image differences, and the performance of the trained models on the real and synthetic datasets (Chapter~\ref{sec:det-model-evaluation}).
The first section delves into the survey results, providing detailed insights into the participants' ability to
distinguish between real and synthetic images.
The second section offers a visual comparison of the generated and real images, highlighting key differences in
color, contrast, brightness, and texture.
The third section will present the performance of the trained models (as described in Chapter~\ref{sec:det-model-selection}) on the
various datasets (outlined in Chapter~\ref{sec:det-dataset-acquisition}).


\section{Survey Results}\label{sec:survey-results}
The survey engaged 11 imaging experts and biologists from Synentec GmbH,
leveraging their professional expertise in microscopy and cell biology.
Each participant was tasked with classifying 30 images, resulting in a total of 330 individual classifications.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{images/survey_results/norm_confusion_matrix}
    \caption{Normalized confusion matrix showing the distribution of correct and incorrect classifications for
    real and generated microscopy images.}
    \label{fig:conf-matrix}
\end{figure}
The confusion matrix (Fig.~\ref{fig:conf-matrix}) offers a concise overview of the participants' ability
to distinguish between real and generated microscopy images.
Out of 330 total classifications, 165 were correct, yielding an overall accuracy of 50\%.
This even split between correct and incorrect classifications suggests a significant challenge
in reliably differentiating between the two image types.
Notably, 54\% of generated images were correctly identified as synthetic, while 46\% were misclassified as real.
Conversely, only 42\% of real images were correctly identified, with 58\% misclassified as synthetic.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/survey_results/individual_accuracy}
    \caption{Bar chart displaying accuracy for individual \glslink{ai}{AI}-generated and real images.}
    \label{fig:individual-accuracy}
\end{figure}
The bar chart (Fig.~\ref{fig:individual-accuracy}) provides a more nuanced view of classification accuracy for individual images.
The data reveals substantial variation across the 30 images, with accuracies ranging from approximately 18\% to 90\%.
This wide range underscores the complexity of the task and the diverse characteristics of the images.
Only four images (numbers 9, 13, 22, and 26) achieved an accuracy of 80\% or higher, suggesting these images possessed
distinctive features that made them more easily classifiable.
Conversely, five images (numbers 3, 14, 17, 18, and 21) had accuracies of 30\% or lower.
The majority of images fell within the 35\% to 60\% accuracy range, with no discernible pattern emerging,
even when differentiating by image type.
This distribution further emphasizes the difficulty participants faced in consistently distinguishing between real and generated images.

\begin{figure}[h!]
    \includegraphics[width=\textwidth]{images/survey_results/wordcloud}
    \caption{Word cloud depicting common reasons for classifying an image as generated.}
    \label{fig:wordcloud}
\end{figure}
The word cloud visualization (Fig.~\ref{fig:wordcloud}) offers valuable insights into the specific features
and characteristics that influenced participants' classifications.
The size of each term corresponds to its frequency in the participants' responses,
with larger words indicating more common reasons for classifying an image as generated.
The prominence of terms such as ``cell'', ``background'', and ``edge'' indicates their frequency in participants'
explanations and highlights the key areas of focus during the decision-making process.
Other notable terms like ``pattern'', ``perfect'', ``artifacts'' and ``noise'' suggest common features associated with generated images.
These terms provide a window into the participants' mental models and the visual cues they relied upon to make their judgments.
The word cloud also reveals clusters of related terms that offer additional context.
Words such as ``blurry'', ``quality'', ``shadows'' and ``contrast'' indicate that
image quality and environmental factors played a significant role in participants' assessments.
This suggests that the overall visual fidelity and realistic imperfections were important considerations
in distinguishing between real and generated images.
Similarly, terms like ``pattern'', ``structure'', ``smooth'', and ``surface'' imply that participants
were attuned to the spatial distribution and arrangement of visual elements,
possibly looking for unnatural regularities or repetitions that might indicate a synthetic origin.
The prominence of ``background'' and related terms like ``noise'' and ``artifact'' underscores the importance of the image
context in the classification process.
Participants were not solely focused on the cells themselves but also considered the surrounding
areas and overall image characteristics.
This comprehensive evaluation of images captures the intricacies involved in the task and highlights the diverse visual
elements that influenced the decision-making.

In conclusion, the survey results reveal the intricate challenge of distinguishing between real and generated microscopy
images of single cells.
The near-even split in overall accuracy, combined with the wide range of individual image accuracies and the
diverse terminology used by participants, demonstrates the sophistication of the generated images and the
difficulty of the classification task.
These findings highlight the potential of unconditional diffusion-based image generation techniques to produce
highly realistic microscopy images that can challenge even expert observers.
The results also underscore the importance of considering multiple visual aspects when evaluating image authenticity
and the potential need for more advanced tools or training to reliably differentiate between real
and synthetic microscopy images in scientific contexts.

\section{Visual Comparison of Generated and Real Images}\label{sec:visual-comparison}
The comparison between generated images (Appendix~\ref{tab:gen_images}) and real \gls{gl:brightfield-microscopy} images
(Appendix~\ref{tab:real_images}) reveals subtle differences.
This analysis employs both visual inspection and quantitative metrics to provide a comprehensive evaluation of the
unconditional diffusion model's performance in replicating \gls{gl:brightfield-microscopy} images.
The metric-based approach includes average brightness, average contrast, color channel ratios, and a color bias metric,
which complement and substantiate the visual observations.

The color bias metric is calculated as the difference between the highest and lowest color channel ratios.
The metrics for the generated images were calculated using the 2500 images included in the \textit{scc\_50} dataset,
whereas the metrics for the real images were calculated using the entire \textit{scc\_real} dataset.
The images were converted from a float range of 0--1 to an integer range of 0--255 for the metric calculations.

One of the most striking differences is the overall color palette.
The generated images exhibit a faint of greenish tint that seems to be overlaid on top of the actual images.
This observation is supported by the metric-based analysis, which reveals a very small color bias of 0.0008 in the generated images
compared to 0.0000 in the real images.
Although this difference is small, it aligns with the perceived color tint.
The color channel ratios (R: 0.3330, G: 0.3338, B: 0.3332) for generated images deviate slightly
from the ideal grayscale distribution of 0.3333 for each channel, further confirming the subtle color variations
introduced by the diffusion model.

In contrast, the real microscopy images maintain a consistent grayscale palette, typical of standard \gls{gl:brightfield-microscopy},
as evidenced by their perfect 0.3333 ratio across all color channels.
This difference in color range suggests that the diffusion model may have introduced some color variations not
present in the original training data.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/contrast_hist}
    \caption{Density histogram depicting the contrast levels of generated and real images.}
    \label{fig:contrast-hist}
\end{figure}
The contrast levels also differ between the two sets, both visually and metrically (Fig.~\ref{fig:contrast-hist}).
The generated images have a lower average contrast value of 8.21, whereas real images have an average contrast of 14.51,
so nearly twice as high.
This discrepancy is also slightly visible in the generated images, which appear a little bit washed out or lacking in detail compared to the real images.
The real images, on the other hand, exhibit a higher contrast range, with more pronounced differences between light and dark areas,
hinting at clearer and more defined cell boundaries.

Brightness levels follow a similar pattern to contrast (Fig.~\ref{fig:brightness-hist}).
The metric-based analysis reveals an average brightness of 206.1 for real images compared to 132.21 for generated images,
indicating a noticeable overexposure in the real images.
Ideally, the brightness levels should be scattered around the middle of the available range (128) to avoid overexposure
or underexposure for grayscale images with 8-bit depth (pixel values of 0--255).
The overexposure in the real images can lead to loss of detail and information in the brighter areas,
which is less likely in the generated images due to their better balanced brightness levels.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{images/brightness_hist}
    \caption{Density histogram illustrating the brightness levels of generated and real images.}
    \label{fig:brightness-hist}
\end{figure}

The texture is another area where differences are apparent.
The generated images sometimes display a subtle, almost canvas-like structure or noise across the background,
which is rarely that noticeable in the real microscopy images.
This added texture in the generated set might be an artifact of the diffusion process or an attempt
by the model to replicate microscopic details it interpreted from the training data.

Lastly, as mentioned in chapter~\ref{subsec:pre-processing}, the model was able to reliably generate images with the same 16 pixel
black padding.
This padding can be generated on every side and serves as a clear indicator of the model's ability to replicate this specific feature,
ensuring consistency across the synthetic and real dataset.

These differences, both visual and metric-based, highlight both the capabilities and current limitations of the unconditional
diffusion model in replicating \gls{gl:brightfield-microscopy} images.
While the model has captured many key features, there are still slight differences that distinguish the
generated images from their real counterparts.

The metric-based analysis provides quantitative support for many of the visual observations,
particularly regarding brightness, contrast, and color variations.
However, it also reveals that some perceived differences, such as the color tint, are subtler than they might appear visually,
giving an initial explanation for the difficulty in distinguishing between real and generated images in the survey.
Generally, this underscores the value of combining both approaches for a more comprehensive evaluation.

\section{Single Cell Detection Performance}\label{sec:single-cell-detection-performance}
A detailed comparison of the performance metrics for all evaluated models across the different training datasets can be found
in Tab.~\ref{tab:model-performances}.
Several key observations can be made from the performance data:
\begin{enumerate}
    \item Inference Speed: The \gls{yolo}v8 models demonstrate the fastest inference times, ranging from approximately 17ms to 21ms.
    \gls{yolo}v9 models show slightly slower inference, with the compact version at ~23ms and the efficient version at ~34ms.
    The \gls{rtdetr} models have the slowest inference times, with the large version at ~34ms and the extra-large at ~40ms.
    \item Model Size and Parameters: As expected, there is a trade-off between model size and performance.
    The \gls{yolo}v8s model has the smallest footprint at 42.7MB, while the \gls{rtdetr}-x model is the largest at 257.9MB\@.
    This variation in model size corresponds to the number of parameters,
    ranging from 11.2M for \gls{yolo}v8s to 68.2M for \gls{yolo}v8 extra-large.
    \item \gls{map} Performance: The \gls{map} metrics reveal interesting patterns across the different \gls{iou} thresholds and training datasets.
    Generally, the models trained on the real dataset slightly outperform those trained on synthetic datasets,
    particularly at higher \gls{iou} thresholds.
    However, the performance gap narrows at lower \gls{iou} thresholds, with some synthetic-trained models achieving
    comparable or even slightly better results in certain cases.
\end{enumerate}
Analyzing the requirements of a specific use case is crucial when selecting the most suitable model for single cell detection.
The trade-offs between inference speed, model size, and detection accuracy must be carefully considered to align
with the project's constraints and objectives.
Ultimately, the choice of model should be tailored to the specific needs of the application,
balancing performance metrics with practical considerations such as computational resources and real-time processing requirements.

\newpage

\begin{table}[h!]
    \centering
    \caption{Detailed comparison of the performance metrics for all evaluated models across the different training datasets.}
    \label{tab:model-performances}
    \begin{tabular}{l|l|P{1.55cm}|P{1.25cm}|P{1.5cm}|P{1.15cm}|P{1.15cm}|P{1.15cm}}
        \toprule
        Model                         & Dataset            & Inference Speed                 & Total Params              & Model Size                  & mAP @50                   & mAP @75                   & mAP @50:95                \\
        \midrule
        \multirow[t]{4}{*}{YOLOv8s}   & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 17ms} & \multirow[t]{4}{*}{11.2M} & \multirow[t]{4}{*}{42.7MB}   & 0.8947                    & \cellcolor{blue!10}0.8095  & \cellcolor{blue!10}0.6557     \\
        & \textit{scc\_10}   &                                 &                           &                             & 0.8941                    & 0.8053                    & 0.6470                    \\
        & \textit{scc\_30}   &                                 &                           &                             & \cellcolor{blue!10}0.9043 & 0.8079                    & 0.6448                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.8932                    & 0.7891                    & 0.6325                    \\
        \midrule
        \multirow[t]{4}{*}{YOLOv8m}   & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 19ms} & \multirow[t]{4}{*}{25.9M} & \multirow[t]{4}{*}{98.9MB}   & \cellcolor{blue!10}0.9038  & \cellcolor{blue!10}0.8203  & \cellcolor{blue!10}0.6637     \\
        & \textit{scc\_10}   &                                 &                           &                             & 0.9035                    & 0.8093                    & 0.6558                    \\
        & \textit{scc\_30}   &                                 &                           &                             & 0.8945                    & 0.8076                    & 0.6462                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.8930                    & 0.8040                    & 0.6456                    \\
        \midrule
        \multirow[t]{4}{*}{YOLOv8x}   & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 21ms} & \multirow[t]{4}{*}{68.2M} & \multirow[t]{4}{*}{260.5MB}  & \cellcolor{blue!10}0.9038  & \cellcolor{blue!10}0.8120  & \cellcolor{blue!10}0.6639     \\
        & \textit{scc\_10}   &                                 &                           &                             & 0.9032                    & 0.8055                    & 0.6531                    \\
        & \textit{scc\_30}   &                                 &                           &                             & 0.9031                    & 0.8085                    & 0.6549                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.8935                    & 0.7961                    & 0.6425                    \\
        \midrule
        \multirow[t]{4}{*}{YOLOv9c}   & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 23ms} & \multirow[t]{4}{*}{25.5M} & \multirow[t]{4}{*}{97.8MB}   & \cellcolor{blue!10}0.9047  & \cellcolor{blue!10}0.8215  & \cellcolor{blue!10}0.6645     \\
        & \textit{scc\_10}   &                                 &                           &                             & 0.9042                    & 0.8070                    & 0.6531                    \\
        & \textit{scc\_30}   &                                 &                           &                             & 0.9042                    & 0.8106                    & 0.6568                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.8935                    & 0.7951                    & 0.6422                    \\
        \midrule
        \multirow[t]{4}{*}{YOLOv9e}   & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 34ms} & \multirow[t]{4}{*}{58.1M} & \multirow[t]{4}{*}{222.4MB} & 0.9037  & \cellcolor{blue!10}0.8201  & 0.6629     \\
        & \textit{scc\_10}   &                                 &                           &                             & \cellcolor{blue!10}0.9041 & 0.8141                    & \cellcolor{blue!10}0.6650 \\
        & \textit{scc\_30}   &                                 &                           &                             & 0.8946                    & 0.8069                    & 0.6485                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.8938                    & 0.8037                    & 0.6453                    \\
        \midrule
        \multirow[t]{4}{*}{RT-DETR-l} & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 34ms} & \multirow[t]{4}{*}{33.0M} & \multirow[t]{4}{*}{126.0MB}  & 0.9146  & \cellcolor{blue!10}0.8169  & \cellcolor{blue!10}0.6614     \\
        & \textit{scc\_10}   &                                 &                           &                             & \cellcolor{blue!10}0.9147 & 0.8071                    & 0.6574                    \\
        & \textit{scc\_30}   &                                 &                           &                             & 0.9017                    & 0.7780                    & 0.6240                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.9036                    & 0.7843                    & 0.6298                    \\
        \midrule
        \multirow[t]{4}{*}{RT-DETR-x} & \textit{scc\_real} & \multirow[t]{4}{*}{$\sim$ 40ms} & \multirow[t]{4}{*}{67.5M} & \multirow[t]{4}{*}{257.9MB}  & \cellcolor{blue!10}0.9164  & \cellcolor{blue!10}0.8257  & \cellcolor{blue!10}0.6748     \\
        & \textit{scc\_10}   &                                 &                           &                             & 0.9144                    & 0.8083                    & 0.6565                    \\
        & \textit{scc\_30}   &                                 &                           &                             & 0.9032                    & 0.7830                    & 0.6328                    \\
        & \textit{scc\_50}   &                                 &                           &                             & 0.9045                    & 0.8032                    & 0.6437                    \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Quantitative Metrics}\label{subsec:analysis-of-map-metrics}
\subsubsection*{mAP@50}
Fig.~\ref{fig:mAP50-results} illustrates the mAP@50 values for all models across the four datasets.
At this lower \gls{iou} threshold, the performance differences between models trained on real and synthetic datasets are relatively small.
In fact, some models trained on synthetic data achieve marginally higher \gls{map}@50 scores than their counterparts trained on real data.
For instance, the \gls{rtdetr}-l model trained on the 10\% synthetic dataset achieves the highest \gls{map}@50 of 0.9147,
slightly outperforming the same model trained on real data (0.9146).
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/mAP50}
    \caption{mAP@50 values for all models trained on the four datasets.}
    \label{fig:mAP50-results}
\end{figure}
Similarly, the \gls{yolo}v9e model trained on the 10\% synthetic dataset achieves a \gls{map}@50 of 0.9041,
marginally higher than the 0.9037 achieved by the real-data trained version.
The \gls{yolo}v8s model trained on the 30\% synthetic dataset also outperforms its real-data counterpart by roughly 1\%.
The largest difference between a model trained on the real data and a model trained on synthetic data is 1.19\% (\gls{rtdetr}-x).
Generally, the \gls{map}@50 values for all models are relatively high, indicating strong performance in detecting cells at this \gls{iou} threshold,
with all models achieving scores above 89\%.
Also interesting to observe is the visible grouping of the models, especially on the real and 10\% synthetic datasets.
As expected by models sizes and architectures, \gls{yolo}v8s performs the worst, whereas the other two \gls{yolo}v8 models and both \gls{yolo}v9 models
perform similarly.
The \gls{rtdetr} models, on the other hand, start as the best performing models, but show the greatest performance drop, particularly
on the 30\% synthetic dataset.

\subsubsection*{mAP@75}
The \gls{map}@75 metric, shown in Fig.~\ref{fig:mAP75-results}, reveals a more pronounced difference between models trained
on real and synthetic datasets.
At this higher \gls{iou} threshold, models trained on real data consistently outperform their synthetic-trained counterparts
across all architectures.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/mAP75}
    \caption{mAP@75 values for all models trained on the four datasets.}
    \label{fig:mAP75-results}
\end{figure}
The \gls{rtdetr}-x model trained on real data achieves the highest \gls{map}@75 of 0.8257,
followed closely by the \gls{yolo}v9 compact model at 0.8215.
The performance gap between real and synthetic-trained models becomes more evident,
with the synthetic models generally showing a decrease in \gls{map}@75 as the proportion of generated images in the training set increases.
The largest gap between a real and synthetic-trained model grew to 4.27\% (\gls{rtdetr}-x).
Additionally, both transformer models (\gls{rtdetr}) perform better on 50\% synthetic data compared to 30\% synthetic data.
As for the model grouping visible in the \gls{map}@50 results, the same pattern can not be observed here.
All \gls{yolo} models perform similarly on all datasets, whereas the \gls{rtdetr} models again show a significant performance drop
on the 30\% synthetic dataset.

\subsubsection*{mAP@50:95}
Lastly, the \gls{map}@50:95 metric, illustrated in Fig.~\ref{fig:mAP5095-results}, provides the most comprehensive view of model
performance across a range of \gls{iou} thresholds.
This metric further reinforces the observations made from the \gls{map}@75 analysis.
The \gls{rtdetr}-x model trained on real data achieves the highest \gls{map}@50:95 of 0.6748,
demonstrating its superior performance across various \gls{iou} thresholds.
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{images/mAP50_95}
    \caption{mAP@50:95 values for all models trained on the four datasets.}
    \label{fig:mAP5095-results}
\end{figure}
Interestingly, the \gls{yolo}v9e model trained on the 10\% synthetic dataset achieves the second-highest \gls{map}@50:95 of 0.6650,
slightly outperforming its real-data trained counterpart (0.6629).
As expected by now, the \gls{rtdetr}-x model again shows the largest performance gap between real and synthetic-trained models,
with a difference of 4.2\%.
And again, the \gls{rtdetr} models show a significant performance drop on the 30\% synthetic dataset, which slightly increases
again on the 50\% synthetic dataset (observable for all three metrics).

\subsection{Qualitative Assessment}\label{subsec:qualitative-assessment}
This chapter presents a detailed analysis of the performance of three object detection models \textemdash \gls{yolo}v8s, \gls{yolo}v9e, and \gls{rtdetr}-l \textemdash on sample
batches from the single-cell detection datasets (Appendix~\ref{ch:sample-predictions}).
The analysis focuses on the models' ability to accurately detect and localize cells in \gls{gl:brightfield-microscopy} images,
highlighting both their strengths and limitations.
The limitations of this thesis do not allow for a more detailed analysis of sample detections of every trained model.

\subsubsection*{\gls{yolo}v8s Sample Detections}
\begin{wrapfigure}{r}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{images/overlapping}
    \caption{Overlapping boxes.}
\end{wrapfigure}
The \gls{yolo}v8s model, despite being the smallest and least complex of the three architectures examined,
demonstrated impressive performance on the \textit{scc\_30} dataset.
Fig.~\ref{fig:yolov8s-samples} illustrates the ground truth labels and model predictions for a representative sample batch.
The ground truth labels, as depicted in Fig.~\ref{fig:yolov8s-labels}, showcase the complexity of the cell detection task.
The labeled images contain a variety of cell presentations, including:

\begin{enumerate}
    \item Fully visible cells within the image boundaries
    \item Partially visible cells at the image edges
    \item Overlapping cells with intersecting bounding boxes
\end{enumerate}

This diversity in cell presentation poses significant challenges for object detection models,
requiring them to accurately identify and localize cells under various conditions.
Despite its relative simplicity, the \gls{yolo}v8s model exhibited remarkable performance on the sample batch.
As shown in Fig.~\ref{fig:yolov8s-preds}, the model successfully detected all cells present in the images with 100\% confidence.
This high level of confidence across all detections suggests that the model has
learned robust features for cell identification, even in challenging scenarios such as partial visibility and overlapping cells.
\newpage

\subsubsection*{YOLOv9e Sample Detections}
\begin{wrapfigure}{l}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{images/missing}
    \caption{Missing box.}
\end{wrapfigure}
The \gls{yolo}v9e model, trained on the \textit{scc\_10} dataset, also showcased strong performance in cell detection tasks.
Fig.~\ref{fig:yolov9e-samples} presents the ground truth labels and model predictions for a sample batch from this datasets' test split.
An important observation from the ground truth labels (Fig.~\ref{fig:yolov9e-labels}) is the presence of a labeling inconsistency.
Specifically, in the image located in the first row and third column, one cell lacks a bounding box label.
This missing label highlights a critical aspect of real-world machine learning applications:
datasets are often imperfect, and models must learn to generalize effectively despite such inconsistencies.
The sample batch for the \gls{yolo}v9e model predominantly features fully visible cells, which may present a somewhat easier
detection scenario compared to the \gls{yolo}v8s sample batch.
This difference in sample characteristics underscores the importance of diverse and representative datasets in
training robust object detection models.
The \gls{yolo}v9e model demonstrated excellent performance on the sample batch, as evidenced in Fig.~\ref{fig:yolov9e-preds}.
The model also successfully detected all cells with high confidence, including the cell with the missing label.
This suggests that the \gls{yolo}v9e architecture has developed a strong internal representation of cell features,
allowing it to generalize beyond the provided labels.
Such robustness is crucial for real-world applications where perfect labeling cannot be guaranteed.

\subsubsection*{RT-DETR-l Sample Detections}
\begin{wrapfigure}{r}{0.25\textwidth}
    \centering
    \includegraphics[width=0.25\textwidth]{images/cutoff}
    \caption{Split box.}
\end{wrapfigure}
The \gls{rtdetr}-l model, also trained on the \textit{scc\_10} dataset, exhibited strong performance in cell detection tasks.
Fig.~\ref{fig:rtdetr-samples} illustrates the ground truth labels and model predictions for a sample batch.
The sample batch for this model represents a combination of the previous two batches, by including partially visible cells at image boundaries,
multiple cells per image and a missing label for one of the cells (Fig.~\ref{fig:rtdetr-labels}).
Key observations of the models' performance on this sample batch are as follows (Fig.~\ref{fig:rtdetr-preds}):
\begin{itemize}
    \item Successful detection of all visible cells, including those with missing labels
    \item High confidence scores for most detections
    \item Detection of the cell with the missing label, albeit with a lower confidence score of 0.3
    \item A false positive detection at the left border of one image, also with a confidence score of 0.3
\end{itemize}
The model's ability to detect the cell with the missing label, similar to the \gls{yolo}v9e model,
indicates robust feature learning and generalization capabilities.
However, the lower confidence score for this detection suggests some uncertainty,
which is reasonable given the lack of a corresponding ground truth label during training.
The potential false positive detection at the image border with a low confidence score of 0.3 highlights an important
consideration in object detection tasks: the trade-off between sensitivity and specificity.
While high sensitivity is crucial for detecting all cells, it may occasionally lead to false positives,
especially in challenging areas such as image boundaries where partial cells or artifacts may be present.

\subsubsection*{Comparative Analysis}
Comparing the performance of the three models \textemdash \gls{yolo}v8s, \gls{yolo}v9e, and \gls{rtdetr}-l \textemdash on their respective
sample batches reveals several important insights:
\begin{itemize}
    \item All three models demonstrated strong cell detection capabilities, successfully identifying cells under various conditions,
    including partial visibility and overlapping instances.
    \item The similar performance of the three models is consistent with their comparable \gls{map} scores, confirming
    that evaluating models based on quantitative metrics and qualitative assessments can provide a comprehensive view of their capabilities.
    \item The models showed robustness to labeling inconsistencies, detecting cells even when ground truth labels were missing.
    This suggests effective feature learning and generalization beyond the provided annotations.
    \item The \gls{yolo}v8s model, despite being the smallest and least complex, achieved 100\% confidence in its
    detections on the sample batch.
    \item This high confidence, however, should be interpreted cautiously and verified across a larger,
    more diverse sample batch to ensure it's not a result of a particularly easy sample batch.
    \item The \gls{yolo}v9e and \gls{rtdetr}-l models, trained on the \textit{scc\_10} dataset, showed similar performance in
    terms of detection accuracy.
    However, the \gls{rtdetr}-l model exhibited more nuanced confidence scoring, particularly for challenging cases
    like the cell with the missing label and the potential false positive at the image border.
    \item The potential false positive detection by the \gls{rtdetr}-l model highlights the ongoing challenge in object
    detection of balancing sensitivity and specificity.
    This trade-off is particularly crucial in medical and biological applications where both false negatives and
    false positives can have significant consequences.
\end{itemize}
These observations underscore the importance of comprehensive evaluation beyond simple accuracy metrics.
Factors such as confidence calibration, robustness to labeling errors, and performance on edge cases (e.g. partially visible cells)
play crucial roles in determining a model's suitability for real-world applications in cell detection and analysis.

#########################################################
################### chapters/08_discussion.tex ###################
#########################################################

\chapter{Discussion}\label{ch:discussion}
The results presented in the previous chapter offer valuable insights into the research questions (Chapter~\ref{sec:research-question}) and have
significant implications for the fields of machine learning, computer vision, and computational biology.

\section{Interpretation of Results}\label{sec:interpretation-of-results}
The survey results involving imaging experts and biologists revealed a remarkable level of difficulty in distinguishing
between real and generated microscopy images.
With an overall accuracy of 50\%, the survey demonstrates the high quality and realism achieved by the unconditional
diffusion model in generating synthetic \gls{gl:brightfield-microscopy} images.
This finding aligns with recent advancements in generative \gls{ai}, particularly in the domain of image synthesis, where
state-of-the-art models have shown remarkable capabilities in replicating complex visual patterns, textures, and sceneries.
The results of the survey further underscore the challenge faced by human experts in this task,
with only a handful of images being consistently classified correctly.
A 50\% accuracy rate resembles random guessing, giving a clear answer to the first sub research question (SRQ1):
the synthetic images are of sufficient quality to be indistinguishable from real images to human experts.

Additionally, the word cloud analysis of participants' reasoning provides valuable insights into the visual cues and features that
experts rely on when evaluating microscopy images.
The prominence of terms such as ``cell'', ``background'', and ``edge'' indicates that both the primary subjects (cells) and the
overall image context play crucial roles in the assessment process.
The frequent mention of image quality-related terms (e.g., ``quality'', ``blurry'', ``contrast'') suggests that the
diffusion model has successfully replicated not only the content but also the typical imperfections and
variations found in real microscopy images.
This level of detail contributes significantly to the realism of the generated images and highlights
the potential of diffusion models in creating diverse and representative synthetic datasets
for machine learning applications in microscopy.
The visual comparison between generated and real images revealed subtle but noteworthy differences,
particularly in color tones, brightness and contrast levels.
The more equally balanced brightness values in the generated images, might actually improve the performance of object detection models.
Whereas lower average contrast values could potentially decrease the performance, as the models might struggle to detect
cells with less pronounced edges.
Additionally, color tints pose a significant problem, since \gls{gl:brightfield-microscopy} is by definition grayscale.
Using synthetic images with color tints in combination with real images introduces a bias to the model, that is not
present in the real data.
There is a need to further investigate the actual impact of these color tints on the performance of object detection models.
These issues also underscores the importance of continued refinement in generative models to more accurately capture
the specific visual properties of \gls{gl:brightfield-microscopy}.
Generally, the differences are hard to spot for the human eye, but might be of technical relevance and importance
for the performance of object detection models.

The performance analysis of various object detection models trained on real and synthetic datasets yielded several significant findings.
Generally, models trained on synthetic datasets achieved comparable performance to those trained on real data,
particularly at lower \gls{iou} thresholds.
This suggests that synthetic images can serve as a viable alternative or supplement to real data for training object
detection models in microscopy applications.
The observation that some models trained on synthetic data marginally outperformed their counterparts trained
on real data at \gls{map}@50 is particularly encouraging, indicating the potential of synthetic data to enhance model
robustness and generalization.
However, the performance gap between real and synthetic-trained models became more pronounced at higher \gls{iou} thresholds,
as evidenced by the \gls{map}@75 and \gls{map}@50:95 metrics.
This trend suggests that while synthetic data can effectively capture general cell features and locations,
it may fall short in replicating the precise boundaries and fine details of cells.
This limitation could be attributed to subtle differences in cell morphology or edge definition between real and generated images,
highlighting an area for future improvement in the image generation process.
Generally, a slight performance decrease was observable when increasing the percentage of synthetic images in the training set.
This decrease was more pronounced in the case of the \gls{rtdetr} models, indicating a greater impact of synthetic data.
These results suggest that sub research question two (SRQ2) can be answered by stating that the proportion of synthetic data
in the training set slightly influences the magnitude and direction of its impact on model performance.
Especially, the transformer-based models seem more sensitive to the proportion of synthetic data in the training set.
But it is important to note that the difference in performance is limited to a few percentage points and might not be
that noticeable in real-world applications.

The qualitative assessment of sample detections from \gls{yolo}v8s, \gls{yolo}v9e, and \gls{rtdetr}-l models provided valuable insights
into their practical performance.
All three models demonstrated strong cell detection capabilities across various challenging scenarios,
including partially visible and overlapping cells.
Their ability to detect cells even in the presence of labeling inconsistencies is particularly noteworthy,
suggesting robust feature learning and generalization beyond the provided annotations.
This resilience to imperfect ground truth data is crucial for real-world applications where labeling errors are common.
But these sample detections and their results have to be analyzed with a grain of salt,
as the sample size is small and might not represent the actual distribution of the data.
They should be analyzed and evaluated in combination with the quantitative results.

The observed differences in confidence scoring between models, particularly the more nuanced approach of the \gls{rtdetr}-l model,
highlight the importance of considering not just detection accuracy but also the reliability of confidence estimates.
The potential false positive detection by \gls{rtdetr}-l at an image border underscores the ongoing challenge of
balancing sensitivity and specificity in object detection tasks, especially in medical and biological applications
where both false negatives and false positives can have significant consequences.

To sum everything up and answer the central research question (RQ):
The introduction of synthetic \gls{gl:brightfield-microscopy} images to a training dataset has an impact on the performance,
although the magnitude of this impact is very limited and not always positive.
Depending on the object detection model, the proportion of synthetic data in the training set, and the evaluation metric,
the performance can slightly increase or decrease.
The results suggest that synthetic data can be a valuable tool for training object detection models in microscopy applications,
providing a cost-effective and scalable alternative to real-world datasets.
However, further refinement of the image generation process and model training strategies is necessary to fully leverage the
potential of synthetic data and really improve the accuracy and robustness of cell detection models.

\section{Impact on Biological Research and Applications}\label{sec:impact-on-biological-research-and-applications}
The findings of this study have significant implications for biological research and applications,
particularly in the realm of \gls{gl:brightfield-microscopy} and cell analysis.
By demonstrating the viability of using synthetic \gls{gl:brightfield-microscopy} images for training object detection models,
this research opens up new possibilities for advancing scientific understanding and improving experimental methodologies.

One of the most immediate impacts is the potential for research groups and laboratories to generate their own data, given sufficient
computational resources and expertise.
This capability could lead to a substantial time and cost savings, as well as increased flexibility in experimental design and data acquisition.
The ability to generate synthetic data is particularly valuable for research groups with limited access to high-quality
microscopy images, democratizing advanced cell detection techniques and potentially accelerating scientific progress
in resource-constrained environments.

The use of generated images also has important implications for \gls{gl:cell-viability} and experimental design.
By reducing the need for fluorescence dyes, which can be harmful to cells, researchers can minimize potential confounding factors
in their experiments.
This approach aligns with the principles of ethical research and may lead to more accurate results in studies of cell behavior and function.

Another significant advantage of synthetic image generation is the ability to create scenarios and cell configurations that are
challenging to achieve in laboratory settings or occur rarely in nature.
This capability addresses a common limitation in machine learning applications, where rare events are often underrepresented in
training data.
By specifically generating these uncommon scenarios, researchers can develop more robust and generalizable cell detection models,
potentially improving performance in real-world applications where such rare events may be critical.

The technique also offers the possibility of generating many images of rare or expensive \glspl{gl:cell-line} in silico.
This could dramatically reduce costs associated with certain types of research and enables studies that might otherwise be
too expensive or logistically challenging to conduct.
Furthermore, the ability to generate diverse datasets could accelerate the process of assay development, drug discovery, and analysis
of cell behavior, potentially leading to faster breakthroughs in biomedical research and pharmaceutical development.

However, it is crucial to note that transparency is paramount when using generated images in research.
To main scientific integrity and avoid ethical issues, researchers should clearly disclose the use of synthetic data in their studies.
This transparency will ensure that the scientific community can properly evaluate the validity and generalizability of findings based on
synthetic data.

In conclusion, the impact of this research on biological studies and applications is far-reaching.
By providing a method to generate high-quality synthetic microscopy images, this work paves the way for more efficient,
cost-effective, and ethically sound research practices.
As the technique is further refined and adopted, it has the potential to accelerate scientific discoveries and innovation in cell
biology and related fields.

\section{Limitations and Future Directions}\label{sec:limitations-and-future-directions}
While this study has yielded promising results, several limitations must be acknowledged, and some paths for future
research remain to be explored.

The most significant limitations and opportunities for improvement lie in the image generation process.
The current architecture of the diffusion model, which produces multichannel images (RGB), introduces undesirable color tints
that are inconsistent with true \gls{gl:brightfield-microscopy}.
Future work should focus on adjusting the model architecture to accept and generate single-channel grayscale images only.
This modification would likely improve the fidelity of the generated images and potentially enhance the performance of object
detection models trained on synthetic data.

The generalizability of the trained diffusion model is another area of concern.
Currently, the model is tailored to a specific biological application and dataset, which limits its broader applicability.
Future research should explore the development of more versatile models, possibly through the use of conditional diffusion techniques,
that can generate images for a wider range of biological applications and cell types.
This would involve gathering and incorporating real microscopy images from diverse sources, including different microscopes
and \glspl{gl:cell-line}, to create a more comprehensive training dataset.

The computational requirements for training the diffusion model posed a significant constraint in this study.
Future work should investigate the relationship between the amount of training data and the quality of generated images,
to determine the optimal balance between computational resources and image fidelity.
Additionally, exploring alternative architectures for the diffusion model could potentially yield improvements in both
efficiency and output quality.

Regarding the evaluation of generated images, it is important to note that the experts and biologists involved in the survey were
aware that some images were generated.
This knowledge may have influenced their judgements, potentially biasing the results.
Future studies that also employ surveys should consider implementing double-blind evaluation protocols and more participants to eliminate
this potential source of bias.

In terms of object detection, there is moderate room for improvement in both data quality and model performance.
Enhancing the quality of label data should be a priority, as this could significantly impact the accuracy of trained models.
Furthermore, a more extensive exploration of the hyperparameter space for the various object detection models could yield
performance improvements.
Future research should also consider evaluating additional state-of-the-art object detection models to identify the most
effective approaches for cell detection in \gls{gl:brightfield-microscopy} images.

The performance discrepancies observed between models trained on real and synthetic data, particularly at higher
\gls{iou} thresholds, suggest that further refinement of the image generation or object detection process is necessary.
Future work should consider improving the replication of fine cellular details and precise boundaries in generated images.
This could involve developing more sophisticated loss functions or incorporating additional biological constraints into the generation process.

Additionally, due to time constrains, as mentioned in Chapter~\ref{subsec:pre-processing}, image patches containing the well edge were
removed from the initial dataset.
Hence, neither the diffusion model was able to generate them, nor the object detection models were trained on them.
Since these artifacts are common in \gls{gl:brightfield-microscopy}, cells are often located, and are especially hard to detect
near the well edge, future research should explore methods to incorporate these challenging scenarios into the training process.
%Otherwise, the models might not be able to reliably detect cells in these regions.

Lastly, while this study focused on \gls{gl:brightfield-microscopy}, future research could further explore the application of similar
techniques to other imaging modalities in biology and medicine.
This could include \gls{gl:fluorescence-microscopy}, electron microscopy, or even medical imaging techniques such as MRI or CT scans.

By pursuing the suggested research directions, future studies can build upon this work
to further advance the field of computational biology and enhance the capabilities of \gls{ai}-assisted microscopy analysis.

#########################################################
################### chapters/09_conclusion.tex ###################
#########################################################

\chapter{Conclusion}\label{ch:conclusion}
This thesis set out to investigate the impact of leveraging unconditional diffusion-based image generation on single cell
detection accuracy in \gls{gl:brightfield-microscopy} images.
The primary objective was to evaluate whether the introduction of synthetic \gls{gl:brightfield-microscopy} images to a
training dataset could positively impact the performance of object detection models in single cell detection tasks.

\section{Summary of Findings}\label{sec:summary-of-findings}
The research yielded several significant findings.
First, the survey results involving imaging experts and biologists revealed a remarkable level of difficulty in
distinguishing between real and generated microscopy images, with an overall accuracy of 50\%.
This demonstrates the high quality and realism achieved by the unconditional diffusion model in generating synthetic \gls{gl:brightfield-microscopy} images.

Second, the performance analysis of various object detection models trained on real and synthetic datasets showed
that models trained on synthetic datasets achieved comparable performance to those trained on real data, particularly at lower \gls{iou} thresholds.
Some models trained on synthetic data even marginally outperformed their counterparts trained on real data at \gls{map}@50.
However, the performance gap between real and synthetic-trained models became more pronounced at higher \gls{iou} thresholds and increased percentage of synthetic data.

Third, the visual comparison between generated and real images revealed subtle but noteworthy differences,
particularly in color tones, contrast levels, and background textures.
While these differences highlight some limitations of the current image generation process,
the increased variability in the synthetic images may actually prove beneficial for developing more robust cell detection models.

\section{Outlook and Recommendations}\label{sec:outlook-and-recommendations}
The findings of this research have significant implications for the fields of machine learning, computer vision, and computational biology.
By demonstrating the viability of using synthetic \gls{gl:brightfield-microscopy} images for training object detection models,
this study opens up new possibilities for advancing scientific understanding and improving experimental methodologies in cell biology and related fields.

The ability to generate high-quality synthetic microscopy images could lead to substantial time and cost savings in research,
as well as increased flexibility in experimental design and data acquisition.
This capability is particularly valuable for research groups with limited access to high-quality microscopy images,
potentially democratizing advanced cell detection techniques and accelerating scientific progress in resource-constrained environments.

However, several limitations and areas for future research remain.
The most significant opportunities for improvement lie in the image generation process.
Future work should focus on adjusting the model architecture to accept and generate single-channel grayscale images,
which could negate the most pronounced visual differences between real and synthetic images: the color tints.
Additionally, exploring the development of more versatile models through conditional diffusion techniques could
expand the applicability of this approach to a wider range of biological applications and cell types.

Further research should also investigate methods to improve the replication of fine cellular details and precise boundaries in generated images,
as well as incorporate challenging scenarios such as cells located near well edges into the training process.
These enhancements could help close the performance gap observed between real and synthetic-trained models at higher \gls{iou} thresholds.

In conclusion, this thesis has demonstrated the potential of using synthetic \gls{gl:brightfield-microscopy} images for cell detection,
while also highlighting the challenges and opportunities for improvement in this emerging field.
As generative \gls{ai} techniques continue to advance, their integration with biological research holds promise for
accelerating scientific discoveries and innovation in cell biology and related disciplines.
The findings of this study provide a foundation for future research in this exciting intersection of computer science
and biology, paving the way for more efficient, cost-effective, and ethically sound research practices in microscopy and cell analysis.